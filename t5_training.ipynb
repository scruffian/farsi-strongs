{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_sparse' from 'torch._C' (c:\\Users\\saaam\\farsi-strongs\\st38\\lib\\site-packages\\torch\\_C.cp38-win_amd64.pyd)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\saaam\\farsi-strongs\\t5_training.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saaam/farsi-strongs/t5_training.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saaam/farsi-strongs/t5_training.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/saaam/farsi-strongs/t5_training.ipynb#ch0000000?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msimpletransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mt5\u001b[39;00m \u001b[39mimport\u001b[39;00m T5Model, T5Args\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saaam/farsi-strongs/t5_training.ipynb#ch0000000?line=5'>6</a>\u001b[0m nmv \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtransformations/NMV_hazm.parquet\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/saaam/farsi-strongs/t5_training.ipynb#ch0000000?line=6'>7</a>\u001b[0m original \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtransformations/original.parquet\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\saaam\\farsi-strongs\\st38\\lib\\site-packages\\simpletransformers\\t5\\__init__.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msimpletransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_args\u001b[39;00m \u001b[39mimport\u001b[39;00m T5Args\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msimpletransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mt5\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mt5_model\u001b[39;00m \u001b[39mimport\u001b[39;00m T5Model\n",
      "File \u001b[1;32mc:\\Users\\saaam\\farsi-strongs\\st38\\lib\\site-packages\\simpletransformers\\config\\model_args.py:8\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m cpu_count\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_default_process_count\u001b[39m():\n\u001b[0;32m     12\u001b[0m     process_count \u001b[39m=\u001b[39m cpu_count() \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m cpu_count() \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\saaam\\farsi-strongs\\st38\\lib\\site-packages\\torch\\__init__.py:537\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[39mreturn\u001b[39;00m path\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    536\u001b[0m \u001b[39m# Shared memory manager needs to know the exact location of manager executable\u001b[39;00m\n\u001b[1;32m--> 537\u001b[0m _C\u001b[39m.\u001b[39;49m_initExtension(manager_path())\n\u001b[0;32m    538\u001b[0m \u001b[39mdel\u001b[39;00m manager_path\n\u001b[0;32m    540\u001b[0m \u001b[39m# Appease the type checker: it can't deal with direct setting of globals().\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39m# Note that we will see \"too many\" functions when reexporting this way; there\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[39m# is not a good way to fix this problem.  Perhaps, try to redesign VariableFunctions\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[39m# so that this import is good enough\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\saaam\\farsi-strongs\\st38\\lib\\site-packages\\torch\\sparse\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Optional, Tuple, List, Union\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m \u001b[39mimport\u001b[39;00m _add_docstr, _sparse  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor\n\u001b[0;32m      8\u001b[0m \u001b[39m# A workaround to support both TorchScript and MyPy:\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_sparse' from 'torch._C' (c:\\Users\\saaam\\farsi-strongs\\st38\\lib\\site-packages\\torch\\_C.cp38-win_amd64.pyd)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "nmv = pd.read_parquet(f\"transformations/NMV_hazm.parquet\")\n",
    "original = pd.read_parquet(f\"transformations/original.parquet\")\n",
    "nt = [\"Matthew\", \"Mark\", \"Luke\", \"John\", \"Acts\", \"Romans\", \"I Corinthians\", \"II Corinthians\", \"Galatians\", \"Ephesians\", \"Philippians\", \"Colossians\", \"I Thessalonians\", \"II Thessalonians\", \"I Timothy\", \"II Timothy\", \"Titus\", \"Philemon\", \"Hebrews\", \"James\", \"I Peter\", \"II Peter\", \"I John\", \"II John\", \"III John\", \"Jude\", \"Revelation of John\"]\n",
    "nmv_ot = nmv.loc[~nmv.book.isin(nt)].copy()\n",
    "nmv_nt = nmv.loc[nmv.book.isin(nt)].copy()\n",
    "original_ot = original.loc[~original.book.isin(nt)].copy()\n",
    "original_nt = original.loc[original.book.isin(nt)].copy()\n",
    "def prepare_translation_datasets(nmv_list, original_list):\n",
    "    def ProcessDF(src, trg):\n",
    "        data = []\n",
    "        for original, fa in zip(trg, src):\n",
    "            data.append([\"translate original to fa\", original, fa])\n",
    "            data.append([\"translate fa to original\", fa, original])\n",
    "\n",
    "        return pd.DataFrame(data, columns=[\"prefix\", \"input_text\", \"target_text\"])\n",
    "        \n",
    "    fa_text = (\n",
    "        nmv_list\n",
    "        .groupby([\"book\", \"idx_chapter\", \"idx_verse\"])\n",
    "        [\"word\"]\n",
    "        .agg(\" \".join)\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    original_text = (\n",
    "        original_list\n",
    "        .groupby([\"book\", \"idx_chapter\", \"idx_verse\"])\n",
    "        [\"eng_word\"]\n",
    "        .agg(\" \".join)\n",
    "        .apply(lambda x:x.replace(\"/\",\"\"))\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    fa_train, fa_test, original_train, original_test = train_test_split(fa_text, original_text, test_size = 0.2)\n",
    "\n",
    "    train_df = ProcessDF(fa_train, original_train)\n",
    "\n",
    "    eval_df = ProcessDF(fa_test, original_test)\n",
    "\n",
    "    return train_df, eval_df\n",
    "ot_train_df, ot_eval_df = prepare_translation_datasets(nmv_ot, original_ot)\n",
    "\n",
    "ot_train_df.to_csv(\"transformations/ot_train.tsv\", sep=\"\\t\")\n",
    "ot_eval_df.to_csv(\"transformations/ot_eval.tsv\", sep=\"\\t\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "train_df = pd.read_csv(\"transformations/ot_train.tsv\", sep=\"\\t\").astype(str)\n",
    "eval_df = pd.read_csv(\"transformations/ot_eval.tsv\", sep=\"\\t\").astype(str)\n",
    "\n",
    "train_df[\"prefix\"] = \"\"\n",
    "eval_df[\"prefix\"] = \"\"\n",
    "\n",
    "model_args = T5Args()\n",
    "model_args.max_seq_length = 96\n",
    "model_args.train_batch_size = 20\n",
    "model_args.eval_batch_size = 20\n",
    "model_args.num_train_epochs = 1\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_steps = 30000\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.fp16 = False\n",
    "model_args.save_steps = -1\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.no_cache = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.preprocess_inputs = False\n",
    "model_args.num_return_sequences = 1\n",
    "model_args.wandb_project = \"MT5 Heb-Fa OT Translation\"\n",
    "\n",
    "model = T5Model(\"mt5\", \"google/mt5-base\", args=model_args, use_cuda=False)\n",
    "\n",
    "model.train_model(train_df, eval_data=eval_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('st38': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2757bf025e96299ff042cfeb287f48c60d9b474690a40aa206164a185f79e800"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
