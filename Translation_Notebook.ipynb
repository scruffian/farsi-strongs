{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging NMV with strongs numbers\n",
    "Getting translations of farsi words to match with english words with strongs number tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert NMV JSON to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Revelation of John', 22, 21]\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>در</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>آغاز</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>،</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>خدا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>آسمانها</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      book  idx_chapter  idx_verse  idx_word     word\n",
       "0  Genesis            0          0         0       در\n",
       "0  Genesis            0          0         1     آغاز\n",
       "0  Genesis            0          0         2        ،\n",
       "0  Genesis            0          0         3      خدا\n",
       "0  Genesis            0          0         4  آسمانها"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hazm import word_tokenize\n",
    "\n",
    "def NMVToDF():\n",
    "    import json\n",
    "    nmv = json.load(open(\"inputs/NMV.json\", encoding=\"utf-8\"))\n",
    "    for book in nmv[\"books\"]:\n",
    "        for idx_chapter, chapter in enumerate(nmv[\"books\"][book]):\n",
    "            for idx_verse, verse in enumerate(chapter):\n",
    "                print([book,idx_chapter+1,idx_verse+1], end=\"\\r\", flush=True)\n",
    "                for idx_word, word in enumerate(word_tokenize(verse)):\n",
    "                    yield pd.DataFrame({\"book\":[book],\"idx_chapter\":[idx_chapter], \"idx_verse\":[idx_verse], \"idx_word\":[idx_word], \"word\":[word]})\n",
    "\n",
    "nmv_df = pd.concat(NMVToDF())\n",
    "nmv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmv_df.to_parquet(\"transformations/NMV_hazm.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert English Bible JSON to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def BibleJSONToDF(version: str):\n",
    "    from tqdm import tqdm\n",
    "    import json\n",
    "    bbl = json.load(open(f\"inputs/{version}.json\", encoding=\"utf-8\"))\n",
    "    for book in tqdm(bbl[\"books\"]):\n",
    "        for idx_chapter, chapter in enumerate(bbl[\"books\"][book]):\n",
    "            for idx_verse, verse in enumerate(chapter):\n",
    "                for idx_word, word in enumerate(verse):\n",
    "                    yield pd.DataFrame(\n",
    "                        {\n",
    "                            \"book\":[book],\n",
    "                            \"idx_chapter\":[idx_chapter], \n",
    "                            \"idx_verse\":[idx_verse], \n",
    "                            \"idx_word\":[idx_word], \n",
    "                            \"eng_word\":[word[0]], \n",
    "                            \"strongs\":[word[1]] if len(word)>1 else [None],\n",
    "                            \"morphology\":[word[2]] if len(word)>2 else [None]\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esv_df = pd.concat(BibleJSONToDF(\"ESV\"))\n",
    "esv_df.to_parquet(\"transformations/ESV.parquet\")\n",
    "esv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [07:06<00:00,  6.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>eng_word</th>\n",
       "      <th>strongs</th>\n",
       "      <th>morphology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In the beginning</td>\n",
       "      <td>H7225</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>God</td>\n",
       "      <td>H430</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>created</td>\n",
       "      <td>H853 H1254</td>\n",
       "      <td>TH8804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>the heaven</td>\n",
       "      <td>H8064</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>and</td>\n",
       "      <td>H853</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      book  idx_chapter  idx_verse  idx_word          eng_word     strongs  \\\n",
       "0  Genesis            0          0         0  In the beginning       H7225   \n",
       "0  Genesis            0          0         1               God        H430   \n",
       "0  Genesis            0          0         2           created  H853 H1254   \n",
       "0  Genesis            0          0         3        the heaven       H8064   \n",
       "0  Genesis            0          0         4               and        H853   \n",
       "\n",
       "  morphology  \n",
       "0       None  \n",
       "0       None  \n",
       "0     TH8804  \n",
       "0       None  \n",
       "0       None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kjv_df = pd.concat(BibleJSONToDF(\"KJV\"))\n",
    "kjv_df.to_parquet(\"transformations/KJV.parquet\")\n",
    "kjv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>eng_word</th>\n",
       "      <th>strongs</th>\n",
       "      <th>morphology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revelation of John</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>you</td>\n",
       "      <td>G5216</td>\n",
       "      <td>P-2GP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revelation of John</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>all</td>\n",
       "      <td>G3956</td>\n",
       "      <td>A-GPM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revelation of John</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revelation of John</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>Amen</td>\n",
       "      <td>G281</td>\n",
       "      <td>HEB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revelation of John</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 book  idx_chapter  idx_verse  idx_word eng_word strongs  \\\n",
       "0  Revelation of John           21         20         7      you   G5216   \n",
       "0  Revelation of John           21         20         8      all   G3956   \n",
       "0  Revelation of John           21         20         9        .    None   \n",
       "0  Revelation of John           21         20        10     Amen    G281   \n",
       "0  Revelation of John           21         20        11        .    None   \n",
       "\n",
       "  morphology  \n",
       "0      P-2GP  \n",
       "0      A-GPM  \n",
       "0       None  \n",
       "0        HEB  \n",
       "0       None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kjv_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Google Cloud translations and synsets approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm\n",
    "import six\n",
    "from google.cloud import translate_v2 as translate\n",
    "from nltk.corpus import stopwords\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def TranslateText(text):\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode(\"utf-8\")\n",
    "    result = translate_client.translate(text, target_language=\"en\", source_language=\"fa\")\n",
    "    return [r[\"translatedText\"] if r[\"translatedText\"] else None for r in result]\n",
    "\n",
    "def RemoveStopWords(phrase):\n",
    "    if type(phrase) == str:\n",
    "        phrase_split = phrase.split(\" \")\n",
    "        if len(phrase_split) > 1:\n",
    "            processed_phrase = \" \".join((wrd for wrd in phrase_split if wrd not in en_stops))\n",
    "            return processed_phrase\n",
    "        else:\n",
    "            return phrase\n",
    "    else:\n",
    "        return phrase\n",
    "\n",
    "def TranslateChapter(df, books):\n",
    "    for book in books:\n",
    "        book_filtered_df = df.loc[df.book==book].copy()\n",
    "        for chapter_id in tqdm(set(book_filtered_df.idx_chapter), desc=book):\n",
    "            chapter_filtered_df = book_filtered_df.loc[book_filtered_df.idx_chapter == chapter_id].copy()\n",
    "\n",
    "            for verse_id in set(chapter_filtered_df.idx_verse):\n",
    "                verse_filtered_df = chapter_filtered_df.loc[chapter_filtered_df.idx_verse == verse_id].copy()\n",
    "\n",
    "                word_list = verse_filtered_df.word_no_punctuation.tolist()\n",
    "\n",
    "                verse_filtered_df[\"translated_word\"] = TranslateText(word_list) \n",
    "                verse_filtered_df[\"translated_word_no_stopwords\"] = verse_filtered_df.translated_word.apply(RemoveStopWords)\n",
    "\n",
    "                yield verse_filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmv = json.load(open(\"inputs/NMV.json\", encoding=\"utf-8\"))\n",
    "books = list(nmv[\"books\"].keys())\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV.parquet\")\n",
    "\n",
    "# For translate api v3\n",
    "# translate_client = translate.TranslationServiceClient.from_service_account_json(\n",
    "#                 'rugged-truck-342720-4470f5b1c878.json'\n",
    "#             )\n",
    "\n",
    "# For translate API v2\n",
    "translate_client = translate.Client.from_service_account_json(\n",
    "    'rugged-truck-342720-4470f5b1c878.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Genesis: 100%|██████████| 50/50 [02:50<00:00,  3.41s/it]\n",
      "Exodus: 100%|██████████| 40/40 [02:12<00:00,  3.32s/it]\n",
      "Leviticus: 100%|██████████| 27/27 [01:34<00:00,  3.52s/it]\n",
      "Numbers: 100%|██████████| 36/36 [02:24<00:00,  4.01s/it]\n",
      "Deuteronomy: 100%|██████████| 34/34 [01:48<00:00,  3.19s/it]\n",
      "Joshua: 100%|██████████| 24/24 [01:13<00:00,  3.04s/it]\n",
      "Judges: 100%|██████████| 21/21 [01:09<00:00,  3.31s/it]\n",
      "Ruth: 100%|██████████| 4/4 [00:09<00:00,  2.31s/it]\n",
      "I Samuel: 100%|██████████| 31/31 [01:28<00:00,  2.86s/it]\n",
      "II Samuel: 100%|██████████| 24/24 [01:17<00:00,  3.21s/it]\n",
      "I Kings: 100%|██████████| 22/22 [01:28<00:00,  4.03s/it]\n",
      "II Kings: 100%|██████████| 25/25 [01:20<00:00,  3.22s/it]\n",
      "I Chronicles: 100%|██████████| 29/29 [01:42<00:00,  3.55s/it]\n",
      "II Chronicles: 100%|██████████| 36/36 [01:30<00:00,  2.53s/it]\n",
      "Ezra: 100%|██████████| 10/10 [00:30<00:00,  3.09s/it]\n",
      "Nehemiah: 100%|██████████| 13/13 [00:44<00:00,  3.39s/it]\n",
      "Esther: 100%|██████████| 10/10 [00:18<00:00,  1.86s/it]\n",
      "Job: 100%|██████████| 42/42 [01:55<00:00,  2.75s/it]\n",
      "Psalms: 100%|██████████| 150/150 [04:24<00:00,  1.76s/it]\n",
      "Proverbs: 100%|██████████| 31/31 [01:37<00:00,  3.13s/it]\n",
      "Ecclesiastes: 100%|██████████| 12/12 [00:23<00:00,  1.98s/it]\n",
      "Song of Solomon: 100%|██████████| 8/8 [00:12<00:00,  1.58s/it]\n",
      "Isaiah: 100%|██████████| 66/66 [02:21<00:00,  2.15s/it]\n",
      "Jeremiah: 100%|██████████| 52/52 [02:29<00:00,  2.87s/it]\n",
      "Lamentations: 100%|██████████| 5/5 [00:16<00:00,  3.24s/it]\n",
      "Ezekiel: 100%|██████████| 48/48 [02:19<00:00,  2.90s/it]\n",
      "Daniel: 100%|██████████| 12/12 [00:39<00:00,  3.26s/it]\n",
      "Hosea: 100%|██████████| 14/14 [00:21<00:00,  1.55s/it]\n",
      "Joel: 100%|██████████| 3/3 [00:07<00:00,  2.67s/it]\n",
      "Amos: 100%|██████████| 9/9 [00:16<00:00,  1.85s/it]\n",
      "Obadiah: 100%|██████████| 1/1 [00:02<00:00,  2.30s/it]\n",
      "Jonah: 100%|██████████| 4/4 [00:04<00:00,  1.23s/it]\n",
      "Micah: 100%|██████████| 7/7 [00:11<00:00,  1.61s/it]\n",
      "Nahum: 100%|██████████| 3/3 [00:04<00:00,  1.65s/it]\n",
      "Habakkuk: 100%|██████████| 3/3 [00:05<00:00,  2.00s/it]\n",
      "Zephaniah: 100%|██████████| 3/3 [00:06<00:00,  2.02s/it]\n",
      "Haggai: 100%|██████████| 2/2 [00:03<00:00,  1.96s/it]\n",
      "Zechariah: 100%|██████████| 14/14 [00:23<00:00,  1.65s/it]\n",
      "Malachi: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]\n",
      "Matthew: 100%|██████████| 28/28 [01:53<00:00,  4.05s/it]\n",
      "Mark: 100%|██████████| 16/16 [01:12<00:00,  4.50s/it]\n",
      "Luke: 100%|██████████| 24/24 [02:03<00:00,  5.14s/it]\n",
      "John: 100%|██████████| 21/21 [01:34<00:00,  4.48s/it]\n",
      "Acts: 100%|██████████| 28/28 [01:47<00:00,  3.84s/it]\n",
      "Romans: 100%|██████████| 16/16 [00:45<00:00,  2.87s/it]\n",
      "I Corinthians: 100%|██████████| 16/16 [00:46<00:00,  2.92s/it]\n",
      "II Corinthians: 100%|██████████| 13/13 [00:27<00:00,  2.08s/it]\n",
      "Galatians: 100%|██████████| 6/6 [00:16<00:00,  2.69s/it]\n",
      "Ephesians: 100%|██████████| 6/6 [00:16<00:00,  2.76s/it]\n",
      "Philippians: 100%|██████████| 4/4 [00:10<00:00,  2.72s/it]\n",
      "Colossians: 100%|██████████| 4/4 [00:10<00:00,  2.58s/it]\n",
      "I Thessalonians: 100%|██████████| 5/5 [00:09<00:00,  1.89s/it]\n",
      "II Thessalonians: 100%|██████████| 3/3 [00:04<00:00,  1.67s/it]\n",
      "I Timothy: 100%|██████████| 6/6 [00:12<00:00,  2.02s/it]\n",
      "II Timothy: 100%|██████████| 4/4 [00:08<00:00,  2.17s/it]\n",
      "Titus: 100%|██████████| 3/3 [00:04<00:00,  1.58s/it]\n",
      "Philemon: 100%|██████████| 1/1 [00:02<00:00,  2.43s/it]\n",
      "Hebrews: 100%|██████████| 13/13 [00:31<00:00,  2.45s/it]\n",
      "James: 100%|██████████| 5/5 [00:11<00:00,  2.30s/it]\n",
      "I Peter: 100%|██████████| 5/5 [00:11<00:00,  2.22s/it]\n",
      "II Peter: 100%|██████████| 3/3 [00:06<00:00,  2.32s/it]\n",
      "I John: 100%|██████████| 5/5 [00:10<00:00,  2.14s/it]\n",
      "II John: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "III John: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "Jude: 100%|██████████| 1/1 [00:02<00:00,  2.67s/it]\n",
      "Revelation of John: 100%|██████████| 22/22 [00:42<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "books_df = pd.concat(TranslateChapter(nmv_df,books))\n",
    "books_df.to_parquet(f\"transformations/NMV_full.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stripping punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmv_df = pd.read_parquet(\"transformations/NMV.parquet\")\n",
    "\n",
    "farsi_chars = \"آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی~۰۱۲۳۴۵۶۷۸۹\"\n",
    "farsi_chars = list(farsi_chars)\n",
    "tqdm.pandas(desc=\"Stripping punctuation\")\n",
    "nmv_df.loc[:,\"word_no_punctuation\"] = nmv_df.word.progress_apply(lambda x:''.join((char for char in x if char in farsi_chars)))\n",
    "nmv_df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying synsets similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def GetSynsets(word):\n",
    "    if word:\n",
    "        return wordnet.synsets(word)\n",
    "\n",
    "def GetEngSynsets(row):\n",
    "    def CompareSynsets(eng_synsets):\n",
    "        # print(eng_synsets)\n",
    "        if row[\"farsi_synsets\"] and eng_synsets:\n",
    "            for t_synset in row[\"farsi_synsets\"]:\n",
    "                if t_synset:\n",
    "                    for e_synset in eng_synsets:\n",
    "                        if e_synset:\n",
    "                            yield t_synset.wup_similarity(e_synset)\n",
    "    # print(row)\n",
    "    eng_verse = esv_df.loc[\n",
    "        (esv_df.book == row[\"book\"]) & \n",
    "        (esv_df.idx_chapter == row[\"idx_chapter\"]) & \n",
    "        (esv_df.idx_verse == row[\"idx_verse\"]) & \n",
    "        (esv_df.strongs.notna())]\n",
    "\n",
    "    eng_verse[\"english_synsets\"] = eng_verse.eng_word.apply(wordnet.synsets)\n",
    "\n",
    "    eng_verse[\"max_similarity\"] = eng_verse.english_synsets.apply(lambda x: max([val for val in CompareSynsets(x)] + [0]))\n",
    "    # print(eng_verse)\n",
    "    eng_verse = eng_verse[[\"idx_word\", \"eng_word\", \"strongs\", \"max_similarity\"]].sort_values(by=\"max_similarity\").tail(n=1)\n",
    "\n",
    "    return eng_verse.values if eng_verse.max_similarity.squeeze()==1 else None\n",
    "\n",
    "nmv_full = pd.read_parquet(\"transformations/NMV_full.parquet\")\n",
    "\n",
    "tqdm.pandas(desc=\"Farsi synsets\")\n",
    "\n",
    "nmv_full[\"farsi_synsets\"] = nmv_full.translated_word_no_stopwords.progress_apply(GetSynsets)\n",
    "esv_df = pd.read_parquet(\"transformations/ESV.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Match to english: 100%|██████████| 100/100 [00:24<00:00,  4.03it/s]\n",
      "C:\\Users\\saaam\\AppData\\Local\\Temp/ipykernel_19404/358563676.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  nmv_partial[\"eng_match\"] = nmv_partial.progress_apply(GetEngSynsets, axis=1)\n"
     ]
    }
   ],
   "source": [
    "nmv_partial = nmv_full.head(n=100)\n",
    "tqdm.pandas(desc=\"Match to english\")\n",
    "nmv_partial[\"eng_match\"] = nmv_partial.progress_apply(GetEngSynsets, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to have about a 35% match rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100 entries, 0 to 0\n",
      "Data columns (total 10 columns):\n",
      " #   Column                        Non-Null Count  Dtype \n",
      "---  ------                        --------------  ----- \n",
      " 0   book                          100 non-null    object\n",
      " 1   idx_chapter                   100 non-null    int64 \n",
      " 2   idx_verse                     100 non-null    int64 \n",
      " 3   idx_word                      100 non-null    int64 \n",
      " 4   word                          100 non-null    object\n",
      " 5   word_no_punctuation           100 non-null    object\n",
      " 6   translated_word               100 non-null    object\n",
      " 7   translated_word_no_stopwords  100 non-null    object\n",
      " 8   farsi_synsets                 100 non-null    object\n",
      " 9   eng_match                     35 non-null     object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 8.6+ KB\n"
     ]
    }
   ],
   "source": [
    "nmv_partial.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing word embeddings approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV_hazm.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec as w2v\n",
    "from hazm import sent_tokenize, word_tokenize\n",
    "import json\n",
    "\n",
    "fa_bible_corpus = nmv_df.word.to_list()\n",
    "fa_bible_sentences = sent_tokenize(\" \".join(fa_bible_corpus)) # nmv_df.groupby([\"book\", \"idx_chapter\", \"idx_verse\"])[\"word\"].agg(list).to_list()\n",
    "fa_bible_sentence_tokens = [word_tokenize(s) for s in fa_bible_sentences]\n",
    "\n",
    "singles = set([w for w in fa_bible_corpus if len(w)==1])\n",
    "punctuation = set([s for s in singles if s not in ['آ', 'و', '\\u200c', '\\u200f']])\n",
    "fa_bible_sentence_tokens_no_punctuation = []\n",
    "for s in fa_bible_sentence_tokens:\n",
    "    s_new = []\n",
    "    for w in s:\n",
    "        if w not in punctuation:\n",
    "            split_w = w.split(\"_\")\n",
    "            if type(split_w) == list:\n",
    "                s_new.extend(split_w)\n",
    "            else:\n",
    "                s_new.append(split_w)\n",
    "    fa_bible_sentence_tokens_no_punctuation.append(s_new)\n",
    "# fa_bible_sentence_tokens_no_punctuation = [[w for w in s if w not in punctuation] for s in fa_bible_sentence_tokens]\n",
    "with open(f\"transformations/NMV_sentences.json\",\"w\",encoding=\"utf8\") as out_f:\n",
    "    json.dump(fa_bible_sentence_tokens_no_punctuation, out_f, ensure_ascii=False)\n",
    "# w2v_model = w2v(fa_bible_sentence_tokens_no_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_wrapper(s):\n",
    "    try:\n",
    "        v = w2v_model.wv.get_vector(s)\n",
    "        return v\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "nmv_df[\"vector\"] = nmv_df.word.apply(get_vector_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>word</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>در</td>\n",
       "      <td>[-0.7641278, -0.5622938, -0.9979115, -0.506839...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>آغاز</td>\n",
       "      <td>[-0.06881103, 0.12053305, 0.06698449, 0.401715...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>،</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>خدا</td>\n",
       "      <td>[1.5973173, 1.1801493, 0.50769883, -0.2529177,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>آسمانها</td>\n",
       "      <td>[0.258966, 0.03336788, -0.028665742, -0.094442...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revelation of John</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>شما</td>\n",
       "      <td>[1.3056972, 1.2187053, 1.3966638, -0.58368856,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revelation of John</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>باد</td>\n",
       "      <td>[0.4446129, 0.04062367, -0.15836331, 0.0167003...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revelation of John</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revelation of John</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>آمین</td>\n",
       "      <td>[0.31524122, 0.36675677, 0.35467657, -0.285576...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revelation of John</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>772044 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  book  idx_chapter  idx_verse  idx_word     word  \\\n",
       "0              Genesis            0          0         0       در   \n",
       "0              Genesis            0          0         1     آغاز   \n",
       "0              Genesis            0          0         2        ،   \n",
       "0              Genesis            0          0         3      خدا   \n",
       "0              Genesis            0          0         4  آسمانها   \n",
       "..                 ...          ...        ...       ...      ...   \n",
       "0   Revelation of John           21         20         5      شما   \n",
       "0   Revelation of John           21         20         6      باد   \n",
       "0   Revelation of John           21         20         7        .   \n",
       "0   Revelation of John           21         20         8     آمین   \n",
       "0   Revelation of John           21         20         9        .   \n",
       "\n",
       "                                               vector  \n",
       "0   [-0.7641278, -0.5622938, -0.9979115, -0.506839...  \n",
       "0   [-0.06881103, 0.12053305, 0.06698449, 0.401715...  \n",
       "0                                                None  \n",
       "0   [1.5973173, 1.1801493, 0.50769883, -0.2529177,...  \n",
       "0   [0.258966, 0.03336788, -0.028665742, -0.094442...  \n",
       "..                                                ...  \n",
       "0   [1.3056972, 1.2187053, 1.3966638, -0.58368856,...  \n",
       "0   [0.4446129, 0.04062367, -0.15836331, 0.0167003...  \n",
       "0                                                None  \n",
       "0   [0.31524122, 0.36675677, 0.35467657, -0.285576...  \n",
       "0                                                None  \n",
       "\n",
       "[772044 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmv_hazm_df = pd.read_parquet(\"transformations/NMV_hazm.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saaam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15648/795859600.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"strongs_sentences.json\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mout_f\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrongs_bible_sentence_tokens_no_punctuation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mstrongs_w2v_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrongs_bible_sentence_tokens_no_punctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w2v' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize as eng_sent_tokenize\n",
    "# from nltk import word_tokenize as eng_word_tokenize\n",
    "from string import punctuation as eng_punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "esv_df = pd.read_parquet(\"transformations/ESV.parquet\")\n",
    "esv_df[\"strongs_punctuation\"] = np.where(\n",
    "    (esv_df.strongs.isna()) & (esv_df.eng_word.isin([a for a in eng_punctuation])), \n",
    "    esv_df.eng_word,\n",
    "    esv_df.strongs\n",
    ")\n",
    "strongs_bible_corpus = esv_df.strongs_punctuation.dropna().to_list()\n",
    "strongs_bible_sentences = eng_sent_tokenize(\" _\".join(strongs_bible_corpus)) # nmv_df.groupby([\"book\", \"idx_chapter\", \"idx_verse\"])[\"word\"].agg(list).to_list()\n",
    "strongs_bible_sentence_tokens = [[w.replace(\"_\",\"\") for w in s.split(\"_\")] for s in strongs_bible_sentences]\n",
    "\n",
    "strongs_bible_sentence_tokens_no_punctuation = [[w for w in s if w not in [a for a in eng_punctuation]] for s in strongs_bible_sentence_tokens]\n",
    "\n",
    "with open(f\"transformations/strongs_sentences.json\",\"w\") as out_f:\n",
    "    json.dump(strongs_bible_sentence_tokens_no_punctuation, out_f)\n",
    "# strongs_w2v_model = w2v(strongs_bible_sentence_tokens_no_punctuation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>eng_word</th>\n",
       "      <th>strongs</th>\n",
       "      <th>strongs_punctuation</th>\n",
       "      <th>strongs_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In the</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>beginning</td>\n",
       "      <td>H7225</td>\n",
       "      <td>H7225</td>\n",
       "      <td>[-0.047161054, -0.007426138, -0.018998928, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>,</td>\n",
       "      <td>None</td>\n",
       "      <td>,</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>God</td>\n",
       "      <td>H430</td>\n",
       "      <td>H430</td>\n",
       "      <td>[-0.9601267, 0.5998295, -0.28738803, 0.9247632...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>created</td>\n",
       "      <td>H1254</td>\n",
       "      <td>H1254</td>\n",
       "      <td>[-0.07384865, 0.014700174, -0.047103573, 0.193...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      book  idx_chapter  idx_verse  idx_word   eng_word strongs  \\\n",
       "0  Genesis            0          0         0     In the    None   \n",
       "0  Genesis            0          0         1  beginning   H7225   \n",
       "0  Genesis            0          0         2          ,    None   \n",
       "0  Genesis            0          0         3        God    H430   \n",
       "0  Genesis            0          0         4    created   H1254   \n",
       "\n",
       "  strongs_punctuation                                     strongs_vector  \n",
       "0                None                                               None  \n",
       "0               H7225  [-0.047161054, -0.007426138, -0.018998928, 0.1...  \n",
       "0                   ,                                               None  \n",
       "0                H430  [-0.9601267, 0.5998295, -0.28738803, 0.9247632...  \n",
       "0               H1254  [-0.07384865, 0.014700174, -0.047103573, 0.193...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def strongs_get_vector_wrapper(s):\n",
    "    try:\n",
    "        v = strongs_w2v_model.wv.get_vector(s)\n",
    "        return v\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "esv_df[\"strongs_vector\"] = esv_df.strongs.apply(strongs_get_vector_wrapper)\n",
    "esv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use following to restrict similarity to words in verse as a key list\n",
    "w2v_model.wv.most_similar_to_given()\n",
    "\n",
    "need to implement https://arxiv.org/pdf/1309.4168.pdf approach to train linear relationship between strongs and farsi wordvec models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data from words matched using index.mjs and wupsimilarity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec as w2v\n",
    "from hazm import sent_tokenize, word_tokenize\n",
    "import json\n",
    "import pandas as pd\n",
    "from string import punctuation as eng_punctuation\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def LoadTrainingPairs(book_name):\n",
    "    with open(f\"transformations/NMV_strongs_{book_name}.json\", encoding=\"utf8\") as f:\n",
    "        nmv_strongs_dict = json.load(f)\n",
    "    training_set = []\n",
    "    for book in nmv_strongs_dict[\"books\"]:\n",
    "        for chapter in nmv_strongs_dict[\"books\"][book]:\n",
    "            for verse in chapter:\n",
    "                for word in verse:\n",
    "                    if len(word)>1:\n",
    "                        if word[1] != None:\n",
    "                            word[0] = \"\".join([a for a in word[0] if a not in list(punctuation)+[a for a in eng_punctuation]+[\"’\"]])\n",
    "                            training_set.append(tuple(word)[::-1])\n",
    "    training_set = list(set(training_set))\n",
    "    return training_set\n",
    "\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV_hazm.parquet\")\n",
    "\n",
    "\n",
    "fa_bible_corpus = nmv_df.word.to_list()\n",
    "fa_bible_sentences = sent_tokenize(\" \".join(fa_bible_corpus)) # nmv_df.groupby([\"book\", \"idx_chapter\", \"idx_verse\"])[\"word\"].agg(list).to_list()\n",
    "fa_bible_sentence_tokens = [word_tokenize(s) for s in fa_bible_sentences]\n",
    "singles = set([w for w in fa_bible_corpus if len(w)==1])\n",
    "punctuation = set([s for s in singles if s not in ['آ', 'و', '\\u200c', '\\u200f']])\n",
    "train = list(\n",
    "    set(\n",
    "        chain(\n",
    "            LoadTrainingPairs(\"Genesis\"),\n",
    "            LoadTrainingPairs(\"Psalms\"),\n",
    "            LoadTrainingPairs(\"Matthew\"),\n",
    "            LoadTrainingPairs(\"I Corinthians\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "with open(f\"transformations/training_pairs.json\", mode=\"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(train, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train bilingual model\n",
    "\n",
    "Use separate virtual environment because of dependency clash between hazm and transvec packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim.models import Word2Vec as w2v\n",
    "from transvec.transformers import TranslationWordVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"transformations/training_pairs.json\", encoding=\"utf8\") as f:\n",
    "    train = json.load(f)\n",
    "with open(\"transformations/NMV_sentences.json\", encoding=\"utf8\") as f:\n",
    "    fa = json.load(f)\n",
    "fa_model = w2v(fa, window=10, min_count=1)\n",
    "with open(\"transformations/strongs_sentences.json\", encoding=\"utf8\") as f:\n",
    "    strongs = json.load(f)\n",
    "strongs_model = w2v(strongs, window = 10, min_count=1)\n",
    "\n",
    "combined_model = TranslationWordVectorizer(strongs_model, fa_model).fit(train)\n",
    "\n",
    "esv_df = pd.read_parquet(\"transformations/ESV.parquet\").reset_index(drop=True)\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV_hazm.parquet\").reset_index(drop=True)\n",
    "\n",
    "def GetStrongsWordSimilarities(row):\n",
    "    from numpy import dot\n",
    "    from gensim import matutils\n",
    "    import numpy as np\n",
    "    print(f\"{round((row/ nmv_df.shape[0])*100, 3)}% complete\", end=\"\\r\", flush=True)\n",
    "    word = nmv_df.iloc[row,4]\n",
    "    \n",
    "    if word in combined_model.sources[0]:\n",
    "        word_vec = combined_model.get_vector(word)\n",
    "        choices_df = esv_df.loc[\n",
    "            (esv_df.book == nmv_df.iloc[row,0]) &\n",
    "            (esv_df.idx_chapter == nmv_df.iloc[row,1]) &\n",
    "            (esv_df.idx_verse == nmv_df.iloc[row,2]) &\n",
    "            (esv_df.strongs.notna()),\n",
    "            [\"idx_word\", \"strongs\"]\n",
    "        ]\n",
    "        choices = zip(choices_df.idx_word, choices_df.strongs)\n",
    "\n",
    "        return [{\"idx_word\": choice[0],\"strongs\":choice[1], \"similarity\":dot(matutils.unitvec(word_vec), matutils.unitvec(combined_model.get_vector(choice[1])))} for choice in choices]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "nmv_df[\"similarities\"] = nmv_df.reset_index()[\"index\"].apply(GetStrongsWordSimilarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(772044, 6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nmv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strongs_model.wv.most_similar_to_given()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TranslationWordVectorizer' object has no attribute 'index_to_key'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\saaam\\farsi-strongs\\Translation_Notebook.ipynb Cell 37'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/saaam/farsi-strongs/Translation_Notebook.ipynb#ch0000031?line=0'>1</a>\u001b[0m combined_model\u001b[39m.\u001b[39;49mindex_to_key\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TranslationWordVectorizer' object has no attribute 'index_to_key'"
     ]
    }
   ],
   "source": [
    "combined_model.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "kjv = json.loads(requests.get(\"https://raw.githubusercontent.com/syncbible/syncbible/gh-pages/bibles/KJV.json\").text)\n",
    "with open(\"inputs/KJV.json\", \"w\") as f:\n",
    "    json.dump(kjv, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "107f9aec7514d8e79cd4921b65cc44c9175ae350aa3c0c7c5debc04712a53a45"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
