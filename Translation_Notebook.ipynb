{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging NMV with strongs numbers\n",
    "Getting translations of farsi words to match with english words with strongs number tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert NMV JSON to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Revelation of John', 22, 21]\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>در</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>آغاز</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>،</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>خدا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>آسمانها</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      book  idx_chapter  idx_verse  idx_word     word\n",
       "0  Genesis            0          0         0       در\n",
       "0  Genesis            0          0         1     آغاز\n",
       "0  Genesis            0          0         2        ،\n",
       "0  Genesis            0          0         3      خدا\n",
       "0  Genesis            0          0         4  آسمانها"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hazm import word_tokenize\n",
    "\n",
    "def NMVToDF():\n",
    "    import json\n",
    "    nmv = json.load(open(\"inputs/NMV.json\", encoding=\"utf-8\"))\n",
    "    for book in nmv[\"books\"]:\n",
    "        for idx_chapter, chapter in enumerate(nmv[\"books\"][book]):\n",
    "            for idx_verse, verse in enumerate(chapter):\n",
    "                print([book,idx_chapter+1,idx_verse+1], end=\"\\r\", flush=True)\n",
    "                for idx_word, word in enumerate(word_tokenize(verse)):\n",
    "                    yield pd.DataFrame({\"book\":[book],\"idx_chapter\":[idx_chapter], \"idx_verse\":[idx_verse], \"idx_word\":[idx_word], \"word\":[word]})\n",
    "\n",
    "nmv_df = pd.concat(NMVToDF())\n",
    "nmv_df.head()\n",
    "nmv_df.to_parquet(\"transformations/NMV_hazm.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert English Bible JSON to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def BibleJSONToDF(version: str):\n",
    "    from tqdm import tqdm\n",
    "    import json\n",
    "    bbl = json.load(open(f\"inputs/{version}.json\", encoding=\"utf-8\"))\n",
    "    for book in tqdm(bbl[\"books\"]):\n",
    "        for idx_chapter, chapter in enumerate(bbl[\"books\"][book]):\n",
    "            for idx_verse, verse in enumerate(chapter):\n",
    "                for idx_word, word in enumerate(verse):\n",
    "                    yield pd.DataFrame(\n",
    "                        {\n",
    "                            \"book\":[book],\n",
    "                            \"idx_chapter\":[idx_chapter], \n",
    "                            \"idx_verse\":[idx_verse], \n",
    "                            \"idx_word\":[idx_word], \n",
    "                            \"eng_word\":[word[0]], \n",
    "                            \"strongs\":[word[1]] if len(word)>1 else [None],\n",
    "                            \"morphology\":[word[2]] if len(word)>2 else [None]\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esv_df = pd.concat(BibleJSONToDF(\"ESV\"))\n",
    "esv_df.to_parquet(\"transformations/ESV.parquet\")\n",
    "esv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [07:06<00:00,  6.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>eng_word</th>\n",
       "      <th>strongs</th>\n",
       "      <th>morphology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In the beginning</td>\n",
       "      <td>H7225</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>God</td>\n",
       "      <td>H430</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>created</td>\n",
       "      <td>H853 H1254</td>\n",
       "      <td>TH8804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>the heaven</td>\n",
       "      <td>H8064</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>and</td>\n",
       "      <td>H853</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      book  idx_chapter  idx_verse  idx_word          eng_word     strongs  \\\n",
       "0  Genesis            0          0         0  In the beginning       H7225   \n",
       "0  Genesis            0          0         1               God        H430   \n",
       "0  Genesis            0          0         2           created  H853 H1254   \n",
       "0  Genesis            0          0         3        the heaven       H8064   \n",
       "0  Genesis            0          0         4               and        H853   \n",
       "\n",
       "  morphology  \n",
       "0       None  \n",
       "0       None  \n",
       "0     TH8804  \n",
       "0       None  \n",
       "0       None  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kjv_df = pd.concat(BibleJSONToDF(\"KJV\"))\n",
    "kjv_df.to_parquet(\"transformations/KJV.parquet\")\n",
    "kjv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Google Cloud translations and synsets approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm\n",
    "import six\n",
    "from google.cloud import translate_v2 as translate\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def GetSynsets(word):\n",
    "    if word:\n",
    "        return wordnet.synsets(word)\n",
    "\n",
    "def GetEngSynsets(row):\n",
    "    def CompareSynsets(eng_synsets):\n",
    "        # print(eng_synsets)\n",
    "        if row[\"farsi_synsets\"] and eng_synsets:\n",
    "            for t_synset in row[\"farsi_synsets\"]:\n",
    "                if t_synset:\n",
    "                    for e_synset in eng_synsets:\n",
    "                        if e_synset:\n",
    "                            yield t_synset.wup_similarity(e_synset)\n",
    "    # print(row)\n",
    "    eng_verse = esv_df.loc[\n",
    "        (esv_df.book == row[\"book\"]) & \n",
    "        (esv_df.idx_chapter == row[\"idx_chapter\"]) & \n",
    "        (esv_df.idx_verse == row[\"idx_verse\"]) & \n",
    "        (esv_df.strongs.notna())]\n",
    "\n",
    "    eng_verse[\"english_synsets\"] = eng_verse.eng_word.apply(wordnet.synsets)\n",
    "\n",
    "    eng_verse[\"max_similarity\"] = eng_verse.english_synsets.apply(lambda x: max([val for val in CompareSynsets(x)] + [0]))\n",
    "    # print(eng_verse)\n",
    "    eng_verse = eng_verse[[\"idx_word\", \"eng_word\", \"strongs\", \"max_similarity\"]].sort_values(by=\"max_similarity\").tail(n=1)\n",
    "\n",
    "    return eng_verse.values if eng_verse.max_similarity.squeeze()==1 else None\n",
    "\n",
    "def TranslateText(text):\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode(\"utf-8\")\n",
    "    result = translate_client.translate(text, target_language=\"en\", source_language=\"fa\")\n",
    "    return [r[\"translatedText\"] if r[\"translatedText\"] else None for r in result]\n",
    "\n",
    "def RemoveStopWords(phrase):\n",
    "    if type(phrase) == str:\n",
    "        phrase_split = phrase.split(\" \")\n",
    "        if len(phrase_split) > 1:\n",
    "            processed_phrase = \" \".join((wrd for wrd in phrase_split if wrd not in en_stops))\n",
    "            return processed_phrase\n",
    "        else:\n",
    "            return phrase\n",
    "    else:\n",
    "        return phrase\n",
    "\n",
    "def TranslateChapter(df, books):\n",
    "    for book in books:\n",
    "        book_filtered_df = df.loc[df.book==book].copy()\n",
    "        for chapter_id in tqdm(set(book_filtered_df.idx_chapter), desc=book):\n",
    "            chapter_filtered_df = book_filtered_df.loc[book_filtered_df.idx_chapter == chapter_id].copy()\n",
    "\n",
    "            for verse_id in set(chapter_filtered_df.idx_verse):\n",
    "                verse_filtered_df = chapter_filtered_df.loc[chapter_filtered_df.idx_verse == verse_id].copy()\n",
    "\n",
    "                word_list = verse_filtered_df.word_no_punctuation.tolist()\n",
    "\n",
    "                verse_filtered_df[\"translated_word\"] = TranslateText(word_list) \n",
    "                verse_filtered_df[\"translated_word_no_stopwords\"] = verse_filtered_df.translated_word.apply(RemoveStopWords)\n",
    "\n",
    "                yield verse_filtered_df\n",
    "\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "nmv = json.load(open(\"inputs/NMV.json\", encoding=\"utf-8\"))\n",
    "books = list(nmv[\"books\"].keys())\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV.parquet\")\n",
    "\n",
    "farsi_chars = \"آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی~۰۱۲۳۴۵۶۷۸۹\"\n",
    "farsi_chars = list(farsi_chars)\n",
    "tqdm.pandas(desc=\"Stripping punctuation\")\n",
    "nmv_df.loc[:,\"word_no_punctuation\"] = nmv_df.word.progress_apply(lambda x:''.join((char for char in x if char in farsi_chars)))\n",
    "\n",
    "# For translate api v3\n",
    "# translate_client = translate.TranslationServiceClient.from_service_account_json(\n",
    "#                 'rugged-truck-342720-4470f5b1c878.json'\n",
    "#             )\n",
    "\n",
    "# For translate API v2\n",
    "translate_client = translate.Client.from_service_account_json(\n",
    "    'rugged-truck-342720-4470f5b1c878.json'\n",
    ")\n",
    "books_df = pd.concat(TranslateChapter(nmv_df,books))\n",
    "books_df.to_parquet(f\"transformations/NMV_full.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying synsets/ wu-palmer similarity method\n",
    "\n",
    "Seems to have about a 35% match rate based on results for Genesis.\n",
    "Very slow. Forgot to time for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nmv_full = pd.read_parquet(\"transformations/NMV_full.parquet\")\n",
    "\n",
    "tqdm.pandas(desc=\"Farsi synsets\")\n",
    "\n",
    "# nmv_full[\"farsi_synsets\"] = nmv_full.translated_word_no_stopwords.progress_apply(GetSynsets)\n",
    "esv_df = pd.read_parquet(\"transformations/ESV.parquet\")\n",
    "\n",
    "nmv_partial = nmv_full.head(n=100)\n",
    "tqdm.pandas(desc=\"Match to english\")\n",
    "nmv_partial[\"eng_match\"] = nmv_partial.progress_apply(GetEngSynsets, axis=1)\n",
    "nmv_partial.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing word embeddings approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec as w2v\n",
    "from hazm import sent_tokenize, word_tokenize\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV_hazm.parquet\")\n",
    "fa_bible_corpus = nmv_df.word.to_list()\n",
    "fa_bible_sentences = sent_tokenize(\" \".join(fa_bible_corpus)) # nmv_df.groupby([\"book\", \"idx_chapter\", \"idx_verse\"])[\"word\"].agg(list).to_list()\n",
    "fa_bible_sentence_tokens = [word_tokenize(s) for s in fa_bible_sentences]\n",
    "\n",
    "singles = set([w for w in fa_bible_corpus if len(w)==1])\n",
    "punctuation = set([s for s in singles if s not in ['آ', 'و', '\\u200c', '\\u200f']])\n",
    "fa_bible_sentence_tokens_no_punctuation = []\n",
    "for s in fa_bible_sentence_tokens:\n",
    "    s_new = []\n",
    "    for w in s:\n",
    "        if w not in punctuation:\n",
    "            split_w = w.split(\"_\")\n",
    "            if type(split_w) == list:\n",
    "                s_new.extend(split_w)\n",
    "            else:\n",
    "                s_new.append(split_w)\n",
    "    fa_bible_sentence_tokens_no_punctuation.append(s_new)\n",
    "# fa_bible_sentence_tokens_no_punctuation = [[w for w in s if w not in punctuation] for s in fa_bible_sentence_tokens]\n",
    "with open(f\"transformations/NMV_sentences.json\",\"w\",encoding=\"utf8\") as out_f:\n",
    "    json.dump(fa_bible_sentence_tokens_no_punctuation, out_f, ensure_ascii=False)\n",
    "# w2v_model = w2v(fa_bible_sentence_tokens_no_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize as eng_sent_tokenize\n",
    "# from nltk import word_tokenize as eng_word_tokenize\n",
    "from string import punctuation as eng_punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "esv_df = pd.read_parquet(\"transformations/ESV.parquet\")\n",
    "esv_df[\"strongs_punctuation\"] = np.where(\n",
    "    (esv_df.strongs.isna()) & (esv_df.eng_word.isin([a for a in eng_punctuation])), \n",
    "    esv_df.eng_word,\n",
    "    esv_df.strongs\n",
    ")\n",
    "strongs_bible_corpus = esv_df.strongs_punctuation.dropna().to_list()\n",
    "strongs_bible_sentences = eng_sent_tokenize(\" _\".join(strongs_bible_corpus)) # nmv_df.groupby([\"book\", \"idx_chapter\", \"idx_verse\"])[\"word\"].agg(list).to_list()\n",
    "strongs_bible_sentence_tokens = [[w.replace(\"_\",\"\") for w in s.split(\"_\")] for s in strongs_bible_sentences]\n",
    "\n",
    "strongs_bible_sentence_tokens_no_punctuation = [[w for w in s if w not in [a for a in eng_punctuation]] for s in strongs_bible_sentence_tokens]\n",
    "\n",
    "with open(f\"transformations/strongs_sentences.json\",\"w\") as out_f:\n",
    "    json.dump(strongs_bible_sentence_tokens_no_punctuation, out_f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data from words matched using index.mjs and wupsimilarity.py\n",
    "\n",
    "need to implement https://arxiv.org/pdf/1309.4168.pdf approach to train linear relationship between strongs and farsi wordvec models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec as w2v\n",
    "from hazm import sent_tokenize, word_tokenize\n",
    "import json\n",
    "import pandas as pd\n",
    "from string import punctuation as eng_punctuation\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def LoadTrainingPairs(book_name):\n",
    "    with open(f\"transformations/NMV_strongs_{book_name}.json\", encoding=\"utf8\") as f:\n",
    "        nmv_strongs_dict = json.load(f)\n",
    "    training_set = []\n",
    "    for book in nmv_strongs_dict[\"books\"]:\n",
    "        for chapter in nmv_strongs_dict[\"books\"][book]:\n",
    "            for verse in chapter:\n",
    "                for word in verse:\n",
    "                    if len(word)>1:\n",
    "                        if word[1] != None:\n",
    "                            word[0] = \"\".join([a for a in word[0] if a not in list(punctuation)+[a for a in eng_punctuation]+[\"’\"]])\n",
    "                            training_set.append(tuple(word)[::-1])\n",
    "    training_set = list(set(training_set))\n",
    "    return training_set\n",
    "\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV_hazm.parquet\")\n",
    "\n",
    "fa_bible_corpus = nmv_df.word.to_list()\n",
    "fa_bible_sentences = sent_tokenize(\" \".join(fa_bible_corpus)) # nmv_df.groupby([\"book\", \"idx_chapter\", \"idx_verse\"])[\"word\"].agg(list).to_list()\n",
    "fa_bible_sentence_tokens = [word_tokenize(s) for s in fa_bible_sentences]\n",
    "singles = set([w for w in fa_bible_corpus if len(w)==1])\n",
    "punctuation = set([s for s in singles if s not in ['آ', 'و', '\\u200c', '\\u200f']])\n",
    "train = list(\n",
    "    set(\n",
    "        chain(\n",
    "            LoadTrainingPairs(\"Genesis\"),\n",
    "            LoadTrainingPairs(\"Psalms\"),\n",
    "            LoadTrainingPairs(\"Matthew\"),\n",
    "            LoadTrainingPairs(\"I Corinthians\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "with open(f\"transformations/training_pairs.json\", mode=\"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(train, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train bilingual model\n",
    "\n",
    "Use separate virtual environment because of dependency clash between hazm and transvec packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002% complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saaam\\AppData\\Local\\Temp\\ipykernel_2572\\349730587.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[\"similarities\"] = test_df.reset_index()[\"index\"].apply(GetStrongsWordSimilarities)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>word</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>در</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>آغاز</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>،</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>خدا</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>آسمانها</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>زمین</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>را</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>آفرید</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>زمین</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>بی‌شکل</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>خالی</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>بود</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>،</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>تاریکی</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>بر</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>روی</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       book  idx_chapter  idx_verse  idx_word     word  \\\n",
       "0   Genesis            0          0         0       در   \n",
       "1   Genesis            0          0         1     آغاز   \n",
       "2   Genesis            0          0         2        ،   \n",
       "3   Genesis            0          0         3      خدا   \n",
       "4   Genesis            0          0         4  آسمانها   \n",
       "5   Genesis            0          0         5        و   \n",
       "6   Genesis            0          0         6     زمین   \n",
       "7   Genesis            0          0         7       را   \n",
       "8   Genesis            0          0         8    آفرید   \n",
       "9   Genesis            0          0         9        .   \n",
       "10  Genesis            0          1         0     زمین   \n",
       "11  Genesis            0          1         1   بی‌شکل   \n",
       "12  Genesis            0          1         2        و   \n",
       "13  Genesis            0          1         3     خالی   \n",
       "14  Genesis            0          1         4      بود   \n",
       "15  Genesis            0          1         5        ،   \n",
       "16  Genesis            0          1         6        و   \n",
       "17  Genesis            0          1         7   تاریکی   \n",
       "18  Genesis            0          1         8       بر   \n",
       "19  Genesis            0          1         9      روی   \n",
       "\n",
       "                                         similarities  \n",
       "0   [{'idx_word': 1, 'strongs': 'H7225', 'similari...  \n",
       "1   [{'idx_word': 1, 'strongs': 'H7225', 'similari...  \n",
       "2                                                None  \n",
       "3   [{'idx_word': 1, 'strongs': 'H7225', 'similari...  \n",
       "4   [{'idx_word': 1, 'strongs': 'H7225', 'similari...  \n",
       "5   [{'idx_word': 1, 'strongs': 'H7225', 'similari...  \n",
       "6   [{'idx_word': 1, 'strongs': 'H7225', 'similari...  \n",
       "7   [{'idx_word': 1, 'strongs': 'H7225', 'similari...  \n",
       "8   [{'idx_word': 1, 'strongs': 'H7225', 'similari...  \n",
       "9                                                None  \n",
       "10  [{'idx_word': 1, 'strongs': 'H776', 'similarit...  \n",
       "11  [{'idx_word': 1, 'strongs': 'H776', 'similarit...  \n",
       "12  [{'idx_word': 1, 'strongs': 'H776', 'similarit...  \n",
       "13  [{'idx_word': 1, 'strongs': 'H776', 'similarit...  \n",
       "14  [{'idx_word': 1, 'strongs': 'H776', 'similarit...  \n",
       "15                                               None  \n",
       "16  [{'idx_word': 1, 'strongs': 'H776', 'similarit...  \n",
       "17  [{'idx_word': 1, 'strongs': 'H776', 'similarit...  \n",
       "18  [{'idx_word': 1, 'strongs': 'H776', 'similarit...  \n",
       "19  [{'idx_word': 1, 'strongs': 'H776', 'similarit...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from gensim.models import Word2Vec as w2v\n",
    "from transvec.transformers import TranslationWordVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def GetStrongsWordSimilarities(row):\n",
    "    from numpy import dot\n",
    "    from gensim import matutils\n",
    "    import numpy as np\n",
    "    print(f\"{round((row/ nmv_df.shape[0])*100, 3)}% complete\", end=\"\\r\", flush=True)\n",
    "    word = nmv_df.iloc[row,4]\n",
    "    \n",
    "    if word in combined_model.sources[0]:\n",
    "        word_vec = combined_model.get_vector(word)\n",
    "        choices_df = esv_df.loc[\n",
    "            (esv_df.book == nmv_df.iloc[row,0]) &\n",
    "            (esv_df.idx_chapter == nmv_df.iloc[row,1]) &\n",
    "            (esv_df.idx_verse == nmv_df.iloc[row,2]) &\n",
    "            (esv_df.strongs.notna()),\n",
    "            [\"idx_word\", \"strongs\"]\n",
    "        ]\n",
    "        choices = zip(choices_df.idx_word, choices_df.strongs)\n",
    "\n",
    "        return [{\"idx_word\": choice[0],\"strongs\":choice[1], \"similarity\":dot(matutils.unitvec(word_vec), matutils.unitvec(combined_model.get_vector(choice[1])))} for choice in choices]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "with open(\"transformations/training_pairs.json\", encoding=\"utf8\") as f:\n",
    "    train = json.load(f)\n",
    "with open(\"transformations/NMV_sentences.json\", encoding=\"utf8\") as f:\n",
    "    fa = json.load(f)\n",
    "fa_model = w2v(fa, window=10, min_count=1)\n",
    "with open(\"transformations/strongs_sentences.json\", encoding=\"utf8\") as f:\n",
    "    strongs = json.load(f)\n",
    "strongs_model = w2v(strongs, window = 10, min_count=1)\n",
    "\n",
    "combined_model = TranslationWordVectorizer(strongs_model, fa_model).fit(train)\n",
    "\n",
    "esv_df = pd.read_parquet(\"transformations/ESV.parquet\").reset_index(drop=True)\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV_hazm.parquet\").reset_index(drop=True)\n",
    "\n",
    "test_df = nmv_df.head(n=20)\n",
    "test_df[\"similarities\"] = test_df.reset_index()[\"index\"].apply(GetStrongsWordSimilarities)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saaam\\AppData\\Local\\Temp\\ipykernel_2572\\1244007009.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[[\"eng_idx_word\", \"strongs\", \"similarity\"]] = pd.DataFrame(test_df.apply(MaxSimilarity, axis=1).to_list())\n"
     ]
    }
   ],
   "source": [
    "def MaxSimilarity(row):\n",
    "    if row[\"similarities\"] == None:\n",
    "        return [None, None, None]\n",
    "        \n",
    "    else:\n",
    "        similarities = pd.DataFrame(row[\"similarities\"])\n",
    "        return similarities.sort_values(by=\"similarity\").tail(n=1).squeeze().to_list()\n",
    "\n",
    "test_df[[\"eng_idx_word\", \"strongs\", \"similarity\"]] = pd.DataFrame(test_df.apply(MaxSimilarity, axis=1).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>word</th>\n",
       "      <th>similarities</th>\n",
       "      <th>eng_idx_word</th>\n",
       "      <th>strongs</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>در</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H7225</td>\n",
       "      <td>0.058196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>آغاز</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H7225</td>\n",
       "      <td>0.057581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>،</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>خدا</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H430</td>\n",
       "      <td>0.050702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>آسمانها</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H430</td>\n",
       "      <td>0.057734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>H776</td>\n",
       "      <td>0.047344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>زمین</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>H776</td>\n",
       "      <td>0.056552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>را</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H430</td>\n",
       "      <td>0.052457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>آفرید</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H7225</td>\n",
       "      <td>0.062160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>زمین</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H922</td>\n",
       "      <td>0.079430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>بی‌شکل</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H922</td>\n",
       "      <td>0.079939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H8414</td>\n",
       "      <td>0.055382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>خالی</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H922</td>\n",
       "      <td>0.063870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>بود</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H922</td>\n",
       "      <td>0.089524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>،</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H8414</td>\n",
       "      <td>0.055382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>تاریکی</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H922</td>\n",
       "      <td>0.063386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>بر</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H922</td>\n",
       "      <td>0.070846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>روی</td>\n",
       "      <td>[{'idx_word': 1, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>H8414</td>\n",
       "      <td>0.060501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       book  idx_chapter  idx_verse  idx_word     word  \\\n",
       "0   Genesis            0          0         0       در   \n",
       "1   Genesis            0          0         1     آغاز   \n",
       "2   Genesis            0          0         2        ،   \n",
       "3   Genesis            0          0         3      خدا   \n",
       "4   Genesis            0          0         4  آسمانها   \n",
       "5   Genesis            0          0         5        و   \n",
       "6   Genesis            0          0         6     زمین   \n",
       "7   Genesis            0          0         7       را   \n",
       "8   Genesis            0          0         8    آفرید   \n",
       "9   Genesis            0          0         9        .   \n",
       "10  Genesis            0          1         0     زمین   \n",
       "11  Genesis            0          1         1   بی‌شکل   \n",
       "12  Genesis            0          1         2        و   \n",
       "13  Genesis            0          1         3     خالی   \n",
       "14  Genesis            0          1         4      بود   \n",
       "15  Genesis            0          1         5        ،   \n",
       "16  Genesis            0          1         6        و   \n",
       "17  Genesis            0          1         7   تاریکی   \n",
       "18  Genesis            0          1         8       بر   \n",
       "19  Genesis            0          1         9      روی   \n",
       "\n",
       "                                         similarities  eng_idx_word strongs  \\\n",
       "0   [{'idx_word': 1, 'strongs': 'H7225', 'similari...           1.0   H7225   \n",
       "1   [{'idx_word': 1, 'strongs': 'H7225', 'similari...           1.0   H7225   \n",
       "2                                                None           NaN    None   \n",
       "3   [{'idx_word': 1, 'strongs': 'H7225', 'similari...           3.0    H430   \n",
       "4   [{'idx_word': 1, 'strongs': 'H7225', 'similari...           3.0    H430   \n",
       "5   [{'idx_word': 1, 'strongs': 'H7225', 'similari...           8.0    H776   \n",
       "6   [{'idx_word': 1, 'strongs': 'H7225', 'similari...           8.0    H776   \n",
       "7   [{'idx_word': 1, 'strongs': 'H7225', 'similari...           3.0    H430   \n",
       "8   [{'idx_word': 1, 'strongs': 'H7225', 'similari...           1.0   H7225   \n",
       "9                                                None           NaN    None   \n",
       "10  [{'idx_word': 1, 'strongs': 'H776', 'similarit...           5.0    H922   \n",
       "11  [{'idx_word': 1, 'strongs': 'H776', 'similarit...           5.0    H922   \n",
       "12  [{'idx_word': 1, 'strongs': 'H776', 'similarit...           3.0   H8414   \n",
       "13  [{'idx_word': 1, 'strongs': 'H776', 'similarit...           5.0    H922   \n",
       "14  [{'idx_word': 1, 'strongs': 'H776', 'similarit...           5.0    H922   \n",
       "15                                               None           NaN    None   \n",
       "16  [{'idx_word': 1, 'strongs': 'H776', 'similarit...           3.0   H8414   \n",
       "17  [{'idx_word': 1, 'strongs': 'H776', 'similarit...           5.0    H922   \n",
       "18  [{'idx_word': 1, 'strongs': 'H776', 'similarit...           5.0    H922   \n",
       "19  [{'idx_word': 1, 'strongs': 'H776', 'similarit...           3.0   H8414   \n",
       "\n",
       "    similarity  \n",
       "0     0.058196  \n",
       "1     0.057581  \n",
       "2          NaN  \n",
       "3     0.050702  \n",
       "4     0.057734  \n",
       "5     0.047344  \n",
       "6     0.056552  \n",
       "7     0.052457  \n",
       "8     0.062160  \n",
       "9          NaN  \n",
       "10    0.079430  \n",
       "11    0.079939  \n",
       "12    0.055382  \n",
       "13    0.063870  \n",
       "14    0.089524  \n",
       "15         NaN  \n",
       "16    0.055382  \n",
       "17    0.063386  \n",
       "18    0.070846  \n",
       "19    0.060501  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get the most similar scores without duplicating strongs words?\n",
    "\n",
    "Could use numpy.meshgrid to get all combinations of word indices for a verse, join to fa word and strongs word, calculate similarity for each combination.\n",
    "Need to then discard all but the set of complete indices that totals the highest similarity score. Not sure how to do that yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [0, 2],\n",
       "       [0, 3],\n",
       "       [0, 4],\n",
       "       [0, 5],\n",
       "       [0, 6],\n",
       "       [0, 7],\n",
       "       [0, 8],\n",
       "       [0, 9],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [1, 2],\n",
       "       [1, 3],\n",
       "       [1, 4],\n",
       "       [1, 5],\n",
       "       [1, 6],\n",
       "       [1, 7],\n",
       "       [1, 8],\n",
       "       [1, 9],\n",
       "       [2, 0],\n",
       "       [2, 1],\n",
       "       [2, 2],\n",
       "       [2, 3],\n",
       "       [2, 4],\n",
       "       [2, 5],\n",
       "       [2, 6],\n",
       "       [2, 7],\n",
       "       [2, 8],\n",
       "       [2, 9],\n",
       "       [3, 0],\n",
       "       [3, 1],\n",
       "       [3, 2],\n",
       "       [3, 3],\n",
       "       [3, 4],\n",
       "       [3, 5],\n",
       "       [3, 6],\n",
       "       [3, 7],\n",
       "       [3, 8],\n",
       "       [3, 9],\n",
       "       [4, 0],\n",
       "       [4, 1],\n",
       "       [4, 2],\n",
       "       [4, 3],\n",
       "       [4, 4],\n",
       "       [4, 5],\n",
       "       [4, 6],\n",
       "       [4, 7],\n",
       "       [4, 8],\n",
       "       [4, 9],\n",
       "       [5, 0],\n",
       "       [5, 1],\n",
       "       [5, 2],\n",
       "       [5, 3],\n",
       "       [5, 4],\n",
       "       [5, 5],\n",
       "       [5, 6],\n",
       "       [5, 7],\n",
       "       [5, 8],\n",
       "       [5, 9],\n",
       "       [6, 0],\n",
       "       [6, 1],\n",
       "       [6, 2],\n",
       "       [6, 3],\n",
       "       [6, 4],\n",
       "       [6, 5],\n",
       "       [6, 6],\n",
       "       [6, 7],\n",
       "       [6, 8],\n",
       "       [6, 9],\n",
       "       [7, 0],\n",
       "       [7, 1],\n",
       "       [7, 2],\n",
       "       [7, 3],\n",
       "       [7, 4],\n",
       "       [7, 5],\n",
       "       [7, 6],\n",
       "       [7, 7],\n",
       "       [7, 8],\n",
       "       [7, 9],\n",
       "       [8, 0],\n",
       "       [8, 1],\n",
       "       [8, 2],\n",
       "       [8, 3],\n",
       "       [8, 4],\n",
       "       [8, 5],\n",
       "       [8, 6],\n",
       "       [8, 7],\n",
       "       [8, 8],\n",
       "       [8, 9],\n",
       "       [9, 0],\n",
       "       [9, 1],\n",
       "       [9, 2],\n",
       "       [9, 3],\n",
       "       [9, 4],\n",
       "       [9, 5],\n",
       "       [9, 6],\n",
       "       [9, 7],\n",
       "       [9, 8],\n",
       "       [9, 9]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "gen1_word_idx_fa = nmv_df.loc[(nmv_df.book == \"Genesis\") & (nmv_df.idx_chapter == 0) & (nmv_df.idx_verse == 0),\"idx_word\"].values\n",
    "\n",
    "gen1_word_idx_en = esv_df.loc[(esv_df.book == \"Genesis\") & (esv_df.idx_chapter == 0) & (esv_df.idx_verse == 0),\"idx_word\"].values\n",
    "np.array(np.meshgrid(gen1_word_idx_fa, gen1_word_idx_en)).T.reshape(-1,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Bible JSON from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "kjv = json.loads(requests.get(\"https://raw.githubusercontent.com/syncbible/syncbible/gh-pages/bibles/KJV.json\").text)\n",
    "with open(\"inputs/KJV.json\", \"w\") as f:\n",
    "    json.dump(kjv, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "107f9aec7514d8e79cd4921b65cc44c9175ae350aa3c0c7c5debc04712a53a45"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
