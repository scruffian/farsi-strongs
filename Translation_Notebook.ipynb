{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging NMV with strongs numbers\n",
    "Getting translations of farsi words to match with english words with strongs number tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert NMV JSON to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Revelation of John', 22, 21]\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>من</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>کتاب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>نخست</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>خود</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>را</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book  idx_chapter  idx_verse  idx_word  word\n",
       "0  Acts            0          0         0    من\n",
       "0  Acts            0          0         1  کتاب\n",
       "0  Acts            0          0         2  نخست\n",
       "0  Acts            0          0         3   خود\n",
       "0  Acts            0          0         4    را"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hazm import word_tokenize\n",
    "\n",
    "def NMVToDF():\n",
    "    import json\n",
    "    nmv = json.load(open(\"inputs/NMV.json\", encoding=\"utf-8\"))\n",
    "    for book in nmv[\"books\"]:\n",
    "        for idx_chapter, chapter in enumerate(nmv[\"books\"][book]):\n",
    "            for idx_verse, verse in enumerate(chapter):\n",
    "                print([book,idx_chapter+1,idx_verse+1], end=\"\\r\", flush=True)\n",
    "                for idx_word, word in enumerate(word_tokenize(verse)):\n",
    "                    yield pd.DataFrame({\"book\":[book],\"idx_chapter\":[idx_chapter], \"idx_verse\":[idx_verse], \"idx_word\":[idx_word], \"word\":[word]})\n",
    "\n",
    "nmv_df = pd.concat(NMVToDF()).sort_values(by=[\"book\", \"idx_chapter\", \"idx_verse\", \"idx_word\"])\n",
    "nmv_df.to_parquet(\"transformations/NMV_hazm.parquet\")\n",
    "nmv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert English Bible JSON to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def BibleJSONToDF(version: str):\n",
    "    from tqdm import tqdm\n",
    "    import json\n",
    "    bbl = json.load(open(f\"inputs/{version}.json\", encoding=\"utf-8\"))\n",
    "    for book in tqdm(bbl[\"books\"]):\n",
    "        for idx_chapter, chapter in enumerate(bbl[\"books\"][book]):\n",
    "            for idx_verse, verse in enumerate(chapter):\n",
    "                for idx_word, word in enumerate(verse):\n",
    "                    yield pd.DataFrame(\n",
    "                        {\n",
    "                            \"book\":[book],\n",
    "                            \"idx_chapter\":[idx_chapter], \n",
    "                            \"idx_verse\":[idx_verse], \n",
    "                            \"idx_word\":[idx_word], \n",
    "                            \"eng_word\":[word[0]], \n",
    "                            \"strongs\":[word[1]] if len(word)>1 else [None],\n",
    "                            \"morphology\":[word[2]] if len(word)>2 else [None]\n",
    "                        }\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esv_df = pd.concat(BibleJSONToDF(\"ESV\"))\n",
    "esv_df.to_parquet(\"transformations/ESV.parquet\")\n",
    "esv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [03:53<00:00,  3.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>eng_word</th>\n",
       "      <th>strongs</th>\n",
       "      <th>morphology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>τον</td>\n",
       "      <td>G3588</td>\n",
       "      <td>T-ASM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>μεν</td>\n",
       "      <td>G3303</td>\n",
       "      <td>PRT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>πρωτον</td>\n",
       "      <td>G4413</td>\n",
       "      <td>A-ASM-S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>λογον</td>\n",
       "      <td>G3056</td>\n",
       "      <td>N-ASM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acts</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>εποιησαμην</td>\n",
       "      <td>G4160</td>\n",
       "      <td>V-AMI-1S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book  idx_chapter  idx_verse  idx_word    eng_word strongs morphology\n",
       "0  Acts            0          0         0         τον   G3588      T-ASM\n",
       "0  Acts            0          0         1         μεν   G3303        PRT\n",
       "0  Acts            0          0         2      πρωτον   G4413    A-ASM-S\n",
       "0  Acts            0          0         3       λογον   G3056      N-ASM\n",
       "0  Acts            0          0         4  εποιησαμην   G4160   V-AMI-1S"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# books are out of order and originals have no sentences... \n",
    "original_df = pd.concat(BibleJSONToDF(\"original\")).sort_values(by=[\"book\", \"idx_chapter\", \"idx_verse\", \"idx_word\"])\n",
    "original_df.to_parquet(\"transformations/original.parquet\")\n",
    "original_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [07:06<00:00,  6.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>eng_word</th>\n",
       "      <th>strongs</th>\n",
       "      <th>morphology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>In the beginning</td>\n",
       "      <td>H7225</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>God</td>\n",
       "      <td>H430</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>created</td>\n",
       "      <td>H853 H1254</td>\n",
       "      <td>TH8804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>the heaven</td>\n",
       "      <td>H8064</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>and</td>\n",
       "      <td>H853</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      book  idx_chapter  idx_verse  idx_word          eng_word     strongs  \\\n",
       "0  Genesis            0          0         0  In the beginning       H7225   \n",
       "0  Genesis            0          0         1               God        H430   \n",
       "0  Genesis            0          0         2           created  H853 H1254   \n",
       "0  Genesis            0          0         3        the heaven       H8064   \n",
       "0  Genesis            0          0         4               and        H853   \n",
       "\n",
       "  morphology  \n",
       "0       None  \n",
       "0       None  \n",
       "0     TH8804  \n",
       "0       None  \n",
       "0       None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kjv_df = pd.concat(BibleJSONToDF(\"KJV\")).sort_values(by=[\"book\", \"idx_chapter\", \"idx_verse\", \"idx_word\"])\n",
    "kjv_df.to_parquet(\"transformations/KJV.parquet\")\n",
    "kjv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Google Cloud translations and synsets approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm\n",
    "import six\n",
    "from google.cloud import translate_v2 as translate\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def GetSynsets(word):\n",
    "    if word:\n",
    "        return wordnet.synsets(word)\n",
    "\n",
    "def GetEngSynsets(row):\n",
    "    def CompareSynsets(eng_synsets):\n",
    "        # print(eng_synsets)\n",
    "        if row[\"farsi_synsets\"] and eng_synsets:\n",
    "            for t_synset in row[\"farsi_synsets\"]:\n",
    "                if t_synset:\n",
    "                    for e_synset in eng_synsets:\n",
    "                        if e_synset:\n",
    "                            yield t_synset.wup_similarity(e_synset)\n",
    "    # print(row)\n",
    "    eng_verse = esv_df.loc[\n",
    "        (esv_df.book == row[\"book\"]) & \n",
    "        (esv_df.idx_chapter == row[\"idx_chapter\"]) & \n",
    "        (esv_df.idx_verse == row[\"idx_verse\"]) & \n",
    "        (esv_df.strongs.notna())]\n",
    "\n",
    "    eng_verse[\"english_synsets\"] = eng_verse.eng_word.apply(wordnet.synsets)\n",
    "\n",
    "    eng_verse[\"max_similarity\"] = eng_verse.english_synsets.apply(lambda x: max([val for val in CompareSynsets(x)] + [0]))\n",
    "    # print(eng_verse)\n",
    "    eng_verse = eng_verse[[\"idx_word\", \"eng_word\", \"strongs\", \"max_similarity\"]].sort_values(by=\"max_similarity\").tail(n=1)\n",
    "\n",
    "    return eng_verse.values if eng_verse.max_similarity.squeeze()==1 else None\n",
    "\n",
    "def TranslateText(text):\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode(\"utf-8\")\n",
    "    result = translate_client.translate(text, target_language=\"en\", source_language=\"fa\")\n",
    "    return [r[\"translatedText\"] if r[\"translatedText\"] else None for r in result]\n",
    "\n",
    "def RemoveStopWords(phrase):\n",
    "    if type(phrase) == str:\n",
    "        phrase_split = phrase.split(\" \")\n",
    "        if len(phrase_split) > 1:\n",
    "            processed_phrase = \" \".join((wrd for wrd in phrase_split if wrd not in en_stops))\n",
    "            return processed_phrase\n",
    "        else:\n",
    "            return phrase\n",
    "    else:\n",
    "        return phrase\n",
    "\n",
    "def TranslateChapter(df, books):\n",
    "    for book in books:\n",
    "        book_filtered_df = df.loc[df.book==book].copy()\n",
    "        for chapter_id in tqdm(set(book_filtered_df.idx_chapter), desc=book):\n",
    "            chapter_filtered_df = book_filtered_df.loc[book_filtered_df.idx_chapter == chapter_id].copy()\n",
    "\n",
    "            for verse_id in set(chapter_filtered_df.idx_verse):\n",
    "                verse_filtered_df = chapter_filtered_df.loc[chapter_filtered_df.idx_verse == verse_id].copy()\n",
    "\n",
    "                word_list = verse_filtered_df.word_no_punctuation.tolist()\n",
    "\n",
    "                verse_filtered_df[\"translated_word\"] = TranslateText(word_list) \n",
    "                verse_filtered_df[\"translated_word_no_stopwords\"] = verse_filtered_df.translated_word.apply(RemoveStopWords)\n",
    "\n",
    "                yield verse_filtered_df\n",
    "\n",
    "en_stops = set(stopwords.words('english'))\n",
    "\n",
    "nmv = json.load(open(\"inputs/NMV.json\", encoding=\"utf-8\"))\n",
    "books = list(nmv[\"books\"].keys())\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV.parquet\")\n",
    "\n",
    "farsi_chars = \"آابپتثجچحخدذرزژسشصضطظعغفقکگلمنوهی~۰۱۲۳۴۵۶۷۸۹\"\n",
    "farsi_chars = list(farsi_chars)\n",
    "tqdm.pandas(desc=\"Stripping punctuation\")\n",
    "nmv_df.loc[:,\"word_no_punctuation\"] = nmv_df.word.progress_apply(lambda x:''.join((char for char in x if char in farsi_chars)))\n",
    "\n",
    "# For translate api v3\n",
    "# translate_client = translate.TranslationServiceClient.from_service_account_json(\n",
    "#                 'rugged-truck-342720-4470f5b1c878.json'\n",
    "#             )\n",
    "\n",
    "# For translate API v2\n",
    "translate_client = translate.Client.from_service_account_json(\n",
    "    'rugged-truck-342720-4470f5b1c878.json'\n",
    ")\n",
    "books_df = pd.concat(TranslateChapter(nmv_df,books))\n",
    "books_df.to_parquet(f\"transformations/NMV_full.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying synsets/ wu-palmer similarity method\n",
    "\n",
    "Seems to have about a 35% match rate based on results for Genesis.\n",
    "Very slow. Forgot to time for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nmv_full = pd.read_parquet(\"transformations/NMV_full.parquet\")\n",
    "\n",
    "tqdm.pandas(desc=\"Farsi synsets\")\n",
    "\n",
    "# nmv_full[\"farsi_synsets\"] = nmv_full.translated_word_no_stopwords.progress_apply(GetSynsets)\n",
    "esv_df = pd.read_parquet(\"transformations/ESV.parquet\")\n",
    "\n",
    "nmv_partial = nmv_full.head(n=100)\n",
    "tqdm.pandas(desc=\"Match to english\")\n",
    "nmv_partial[\"eng_match\"] = nmv_partial.progress_apply(GetEngSynsets, axis=1)\n",
    "nmv_partial.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing word embeddings approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec as w2v\n",
    "from hazm import sent_tokenize, word_tokenize\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: find workaround for lack of sentences in original text json\n",
    "# Will it work to tokenize on words only and build model on one big sentence?\n",
    "# Can text be chunked into arbitrary sentence sizes without affecting model accuracy?\n",
    "\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV_hazm.parquet\")\n",
    "fa_bible_corpus = nmv_df.word.to_list()\n",
    "fa_bible_sentences = sent_tokenize(\" \".join(fa_bible_corpus)) # nmv_df.groupby([\"book\", \"idx_chapter\", \"idx_verse\"])[\"word\"].agg(list).to_list()\n",
    "fa_bible_sentence_tokens = [word_tokenize(s) for s in fa_bible_sentences]\n",
    "\n",
    "singles = set([w for w in fa_bible_corpus if len(w)==1])\n",
    "punctuation = set([s for s in singles if s not in ['آ', 'و', '\\u200c', '\\u200f']])\n",
    "fa_bible_sentence_tokens_no_punctuation = []\n",
    "for s in fa_bible_sentence_tokens:\n",
    "    s_new = []\n",
    "    for w in s:\n",
    "        if w not in punctuation:\n",
    "            split_w = w.split(\"_\")\n",
    "            if type(split_w) == list:\n",
    "                s_new.extend(split_w)\n",
    "            else:\n",
    "                s_new.append(split_w)\n",
    "    fa_bible_sentence_tokens_no_punctuation.append(s_new)\n",
    "# fa_bible_sentence_tokens_no_punctuation = [[w for w in s if w not in punctuation] for s in fa_bible_sentence_tokens]\n",
    "with open(f\"transformations/NMV_sentences.json\",\"w\",encoding=\"utf8\") as out_f:\n",
    "    json.dump(fa_bible_sentence_tokens_no_punctuation, out_f, ensure_ascii=False)\n",
    "# w2v_model = w2v(fa_bible_sentence_tokens_no_punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize as eng_sent_tokenize\n",
    "# from nltk import word_tokenize as eng_word_tokenize\n",
    "from string import punctuation as eng_punctuation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def ParquetToStrongsSentences(eng_version: str):\n",
    "    eng_df = pd.read_parquet(f\"transformations/{eng_version}.parquet\")\n",
    "    eng_df[\"strongs_punctuation\"] = np.where(\n",
    "        (eng_df.strongs.isna()) & (eng_df.eng_word.isin([a for a in eng_punctuation])), \n",
    "        eng_df.eng_word,\n",
    "        eng_df.strongs\n",
    "    )\n",
    "    strongs_bible_corpus = eng_df.strongs_punctuation.dropna().to_list()\n",
    "    strongs_bible_sentences = eng_sent_tokenize(\" \".join([w.replace(\" \", \"_\") for w in strongs_bible_corpus])) # nmv_df.groupby([\"book\", \"idx_chapter\", \"idx_verse\"])[\"word\"].agg(list).to_list()\n",
    "    strongs_bible_sentence_tokens = [[w.replace(\"_\",\" \") for w in re.split(\" /\",s)] for s in strongs_bible_sentences]\n",
    "\n",
    "    strongs_bible_sentence_tokens_no_punctuation = [[w for w in s if w not in [a for a in eng_punctuation] + [\"added\"]] for s in strongs_bible_sentence_tokens]\n",
    "\n",
    "    with open(f\"transformations/{eng_version}_strongs_sentences.json\",\"w\") as out_f:\n",
    "        json.dump(strongs_bible_sentence_tokens_no_punctuation, out_f)\n",
    "\n",
    "ParquetToStrongsSentences(\"original\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data from words matched using index.mjs and wupsimilarity.py\n",
    "\n",
    "implements https://arxiv.org/pdf/1309.4168.pdf approach to train linear relationship between strongs and farsi wordvec models using transvec module\n",
    "\n",
    "consider merging training pairs created from multiple english versions for better coverage. Drop pairs with multiple strongs numbers that were derived from ESV?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec as w2v\n",
    "from hazm import sent_tokenize, word_tokenize\n",
    "import json\n",
    "import pandas as pd\n",
    "from string import punctuation as eng_punctuation\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "def LoadTrainingPairs(eng_version: str, book_name: str):\n",
    "    with open(f\"transformations/NMV_{eng_version}_strongs_{book_name}.json\", encoding=\"utf8\") as f:\n",
    "        nmv_strongs_dict = json.load(f)\n",
    "    training_set = []\n",
    "    for book in nmv_strongs_dict[\"books\"]:\n",
    "        for chapter in nmv_strongs_dict[\"books\"][book]:\n",
    "            for verse in chapter:\n",
    "                for word in verse:\n",
    "                    if len(word)>1:\n",
    "                        if word[1] not in [None, \"added\"]:\n",
    "                            word[0] = \"\".join([a for a in word[0] if a not in list(punctuation)+[a for a in eng_punctuation]+[\"’\"]])\n",
    "                            training_set.append(tuple(word)[::-1])\n",
    "    training_set = list(set(training_set))\n",
    "    return training_set\n",
    "\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV_hazm.parquet\")\n",
    "eng_version = \"KJV\"\n",
    "\n",
    "fa_bible_corpus = nmv_df.word.to_list()\n",
    "fa_bible_sentences = sent_tokenize(\" \".join(fa_bible_corpus)) # nmv_df.groupby([\"book\", \"idx_chapter\", \"idx_verse\"])[\"word\"].agg(list).to_list()\n",
    "fa_bible_sentence_tokens = [word_tokenize(s) for s in fa_bible_sentences]\n",
    "singles = set([w for w in fa_bible_corpus if len(w)==1])\n",
    "punctuation = set([s for s in singles if s not in ['آ', 'و', '\\u200c', '\\u200f']])\n",
    "train = list(\n",
    "    set(\n",
    "        chain(\n",
    "            LoadTrainingPairs(eng_version, \"Genesis\"),\n",
    "            LoadTrainingPairs(eng_version, \"Psalms\"),\n",
    "            LoadTrainingPairs(eng_version, \"Habakkuk\"),\n",
    "            LoadTrainingPairs(eng_version, \"Matthew\"),\n",
    "            LoadTrainingPairs(eng_version, \"I Corinthians\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "with open(f\"transformations/{eng_version}_training_pairs.json\", mode=\"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(train, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train bilingual model\n",
    "\n",
    "Use separate virtual environment because of dependency clash between hazm and transvec packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002% complete\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saaam\\AppData\\Local\\Temp\\ipykernel_15564\\3061733741.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  kjv_test_df[\"similarities\"] = kjv_test_df.reset_index()[\"index\"].apply(GetStrongsWordSimilarities,args=(kjv_df, kjv_combined_model))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>word</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>در</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>آغاز</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>،</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>خدا</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>آسمانها</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>زمین</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>را</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>آفرید</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>زمین</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>بی‌شکل</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>خالی</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>بود</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>،</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>تاریکی</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>بر</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>روی</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       book  idx_chapter  idx_verse  idx_word     word  \\\n",
       "0   Genesis            0          0         0       در   \n",
       "1   Genesis            0          0         1     آغاز   \n",
       "2   Genesis            0          0         2        ،   \n",
       "3   Genesis            0          0         3      خدا   \n",
       "4   Genesis            0          0         4  آسمانها   \n",
       "5   Genesis            0          0         5        و   \n",
       "6   Genesis            0          0         6     زمین   \n",
       "7   Genesis            0          0         7       را   \n",
       "8   Genesis            0          0         8    آفرید   \n",
       "9   Genesis            0          0         9        .   \n",
       "10  Genesis            0          1         0     زمین   \n",
       "11  Genesis            0          1         1   بی‌شکل   \n",
       "12  Genesis            0          1         2        و   \n",
       "13  Genesis            0          1         3     خالی   \n",
       "14  Genesis            0          1         4      بود   \n",
       "15  Genesis            0          1         5        ،   \n",
       "16  Genesis            0          1         6        و   \n",
       "17  Genesis            0          1         7   تاریکی   \n",
       "18  Genesis            0          1         8       بر   \n",
       "19  Genesis            0          1         9      روی   \n",
       "\n",
       "                                         similarities  \n",
       "0   [{'idx_word': 0, 'strongs': 'H7225', 'similari...  \n",
       "1   [{'idx_word': 0, 'strongs': 'H7225', 'similari...  \n",
       "2                                                None  \n",
       "3   [{'idx_word': 0, 'strongs': 'H7225', 'similari...  \n",
       "4   [{'idx_word': 0, 'strongs': 'H7225', 'similari...  \n",
       "5   [{'idx_word': 0, 'strongs': 'H7225', 'similari...  \n",
       "6   [{'idx_word': 0, 'strongs': 'H7225', 'similari...  \n",
       "7   [{'idx_word': 0, 'strongs': 'H7225', 'similari...  \n",
       "8   [{'idx_word': 0, 'strongs': 'H7225', 'similari...  \n",
       "9                                                None  \n",
       "10  [{'idx_word': 0, 'strongs': 'H776', 'similarit...  \n",
       "11  [{'idx_word': 0, 'strongs': 'H776', 'similarit...  \n",
       "12  [{'idx_word': 0, 'strongs': 'H776', 'similarit...  \n",
       "13  [{'idx_word': 0, 'strongs': 'H776', 'similarit...  \n",
       "14  [{'idx_word': 0, 'strongs': 'H776', 'similarit...  \n",
       "15                                               None  \n",
       "16  [{'idx_word': 0, 'strongs': 'H776', 'similarit...  \n",
       "17  [{'idx_word': 0, 'strongs': 'H776', 'similarit...  \n",
       "18  [{'idx_word': 0, 'strongs': 'H776', 'similarit...  \n",
       "19  [{'idx_word': 0, 'strongs': 'H776', 'similarit...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from gensim.models import Word2Vec as w2v\n",
    "from transvec.transformers import TranslationWordVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def GetStrongsWordSimilarities(row: pd.Series, eng_df: pd.DataFrame, combined_model: TranslationWordVectorizer):\n",
    "    from numpy import dot\n",
    "    from gensim import matutils\n",
    "    import numpy as np\n",
    "    print(f\"{round((row/ nmv_df.shape[0])*100, 3)}% complete\", end=\"\\r\", flush=True)\n",
    "    word = nmv_df.iloc[row,4]\n",
    "    \n",
    "    if word in combined_model.sources[0]:\n",
    "        word_vec = combined_model.get_vector(word)\n",
    "        choices_df = eng_df.loc[\n",
    "            (eng_df.book == nmv_df.iloc[row,0]) &\n",
    "            (eng_df.idx_chapter == nmv_df.iloc[row,1]) &\n",
    "            (eng_df.idx_verse == nmv_df.iloc[row,2]) &\n",
    "            (eng_df.strongs.notna()) &\n",
    "            (eng_df.strongs != \"added\"),\n",
    "            [\"idx_word\", \"strongs\"]\n",
    "        ]\n",
    "        choices = zip(choices_df.idx_word, choices_df.strongs)\n",
    "\n",
    "        return [{\"idx_word\": choice[0],\"strongs\":choice[1], \"similarity\":dot(matutils.unitvec(word_vec), matutils.unitvec(combined_model.get_vector(choice[1])))} for choice in choices]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def BuildBilingualModel(eng_version: str):\n",
    "    with open(f\"transformations/{eng_version}_training_pairs.json\", encoding=\"utf8\") as f:\n",
    "        train = json.load(f)\n",
    "    with open(\"transformations/NMV_sentences.json\", encoding=\"utf8\") as f:\n",
    "        fa = json.load(f)\n",
    "    fa_model = w2v(fa, window=10, min_count=1)\n",
    "    with open(f\"transformations/{eng_version}_strongs_sentences.json\", encoding=\"utf8\") as f:\n",
    "        strongs = json.load(f)\n",
    "    strongs_model = w2v(strongs, window = 10, min_count=1)\n",
    "\n",
    "    combined_model = TranslationWordVectorizer(strongs_model, fa_model).fit(train)\n",
    "\n",
    "    return combined_model\n",
    "\n",
    "# esv_df = pd.read_parquet(\"transformations/ESV.parquet\").reset_index(drop=True)\n",
    "kjv_df = pd.read_parquet(\"transformations/KJV.parquet\").reset_index(drop=True)\n",
    "nmv_df = pd.read_parquet(\"transformations/NMV_hazm.parquet\").reset_index(drop=True)\n",
    "kjv_combined_model = BuildBilingualModel(\"KJV\")\n",
    "\n",
    "kjv_test_df = nmv_df.head(n=20)\n",
    "kjv_test_df[\"similarities\"] = kjv_test_df.reset_index()[\"index\"].apply(GetStrongsWordSimilarities,args=(kjv_df, kjv_combined_model))\n",
    "kjv_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saaam\\AppData\\Local\\Temp\\ipykernel_15564\\2632906201.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  kjv_test_df[[\"eng_idx_word\", \"strongs\", \"similarity\"]] = pd.DataFrame(kjv_test_df.apply(MaxSimilarity, axis=1).to_list())\n",
      "C:\\Users\\saaam\\AppData\\Local\\Temp\\ipykernel_15564\\2632906201.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  kjv_test_df[[\"eng_idx_word\", \"strongs\", \"similarity\"]] = pd.DataFrame(kjv_test_df.apply(MaxSimilarity, axis=1).to_list())\n",
      "C:\\Users\\saaam\\AppData\\Local\\Temp\\ipykernel_15564\\2632906201.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  kjv_test_df[[\"eng_idx_word\", \"strongs\", \"similarity\"]] = pd.DataFrame(kjv_test_df.apply(MaxSimilarity, axis=1).to_list())\n"
     ]
    }
   ],
   "source": [
    "def MaxSimilarity(row):\n",
    "    if row[\"similarities\"] == None:\n",
    "        return [None, None, None]\n",
    "        \n",
    "    else:\n",
    "        similarities = pd.DataFrame(row[\"similarities\"])\n",
    "        return similarities.sort_values(by=\"similarity\").tail(n=1).squeeze().to_list()\n",
    "\n",
    "kjv_test_df[[\"eng_idx_word\", \"strongs\", \"similarity\"]] = pd.DataFrame(kjv_test_df.apply(MaxSimilarity, axis=1).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>idx_chapter</th>\n",
       "      <th>idx_verse</th>\n",
       "      <th>idx_word</th>\n",
       "      <th>word</th>\n",
       "      <th>similarities</th>\n",
       "      <th>eng_idx_word</th>\n",
       "      <th>strongs</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>در</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>H853</td>\n",
       "      <td>0.092953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>آغاز</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H7225</td>\n",
       "      <td>0.061042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>،</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>خدا</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H430</td>\n",
       "      <td>0.050172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>آسمانها</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>H430</td>\n",
       "      <td>0.062915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>H853</td>\n",
       "      <td>0.067851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>زمین</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H776</td>\n",
       "      <td>0.061576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>را</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>H776</td>\n",
       "      <td>0.058538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>آفرید</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H7225', 'similari...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>H7225</td>\n",
       "      <td>0.069265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>.</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>زمین</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>H922</td>\n",
       "      <td>0.084571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>بی‌شکل</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>H8415</td>\n",
       "      <td>0.061089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>H8415</td>\n",
       "      <td>0.062495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>خالی</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>H8414</td>\n",
       "      <td>0.071861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>بود</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>H5921</td>\n",
       "      <td>0.035794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>،</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>و</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>H8415</td>\n",
       "      <td>0.062495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>تاریکی</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>H922</td>\n",
       "      <td>0.080812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>بر</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>H922</td>\n",
       "      <td>0.080369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Genesis</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>روی</td>\n",
       "      <td>[{'idx_word': 0, 'strongs': 'H776', 'similarit...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>H922</td>\n",
       "      <td>0.093339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       book  idx_chapter  idx_verse  idx_word     word  \\\n",
       "0   Genesis            0          0         0       در   \n",
       "1   Genesis            0          0         1     آغاز   \n",
       "2   Genesis            0          0         2        ،   \n",
       "3   Genesis            0          0         3      خدا   \n",
       "4   Genesis            0          0         4  آسمانها   \n",
       "5   Genesis            0          0         5        و   \n",
       "6   Genesis            0          0         6     زمین   \n",
       "7   Genesis            0          0         7       را   \n",
       "8   Genesis            0          0         8    آفرید   \n",
       "9   Genesis            0          0         9        .   \n",
       "10  Genesis            0          1         0     زمین   \n",
       "11  Genesis            0          1         1   بی‌شکل   \n",
       "12  Genesis            0          1         2        و   \n",
       "13  Genesis            0          1         3     خالی   \n",
       "14  Genesis            0          1         4      بود   \n",
       "15  Genesis            0          1         5        ،   \n",
       "16  Genesis            0          1         6        و   \n",
       "17  Genesis            0          1         7   تاریکی   \n",
       "18  Genesis            0          1         8       بر   \n",
       "19  Genesis            0          1         9      روی   \n",
       "\n",
       "                                         similarities  eng_idx_word strongs  \\\n",
       "0   [{'idx_word': 0, 'strongs': 'H7225', 'similari...           4.0    H853   \n",
       "1   [{'idx_word': 0, 'strongs': 'H7225', 'similari...           0.0   H7225   \n",
       "2                                                None           NaN    None   \n",
       "3   [{'idx_word': 0, 'strongs': 'H7225', 'similari...           1.0    H430   \n",
       "4   [{'idx_word': 0, 'strongs': 'H7225', 'similari...           1.0    H430   \n",
       "5   [{'idx_word': 0, 'strongs': 'H7225', 'similari...           4.0    H853   \n",
       "6   [{'idx_word': 0, 'strongs': 'H7225', 'similari...           5.0    H776   \n",
       "7   [{'idx_word': 0, 'strongs': 'H7225', 'similari...           5.0    H776   \n",
       "8   [{'idx_word': 0, 'strongs': 'H7225', 'similari...           0.0   H7225   \n",
       "9                                                None           NaN    None   \n",
       "10  [{'idx_word': 0, 'strongs': 'H776', 'similarit...           4.0    H922   \n",
       "11  [{'idx_word': 0, 'strongs': 'H776', 'similarit...           9.0   H8415   \n",
       "12  [{'idx_word': 0, 'strongs': 'H776', 'similarit...           9.0   H8415   \n",
       "13  [{'idx_word': 0, 'strongs': 'H776', 'similarit...           2.0   H8414   \n",
       "14  [{'idx_word': 0, 'strongs': 'H776', 'similarit...          14.0   H5921   \n",
       "15                                               None           NaN    None   \n",
       "16  [{'idx_word': 0, 'strongs': 'H776', 'similarit...           9.0   H8415   \n",
       "17  [{'idx_word': 0, 'strongs': 'H776', 'similarit...           4.0    H922   \n",
       "18  [{'idx_word': 0, 'strongs': 'H776', 'similarit...           4.0    H922   \n",
       "19  [{'idx_word': 0, 'strongs': 'H776', 'similarit...           4.0    H922   \n",
       "\n",
       "    similarity  \n",
       "0     0.092953  \n",
       "1     0.061042  \n",
       "2          NaN  \n",
       "3     0.050172  \n",
       "4     0.062915  \n",
       "5     0.067851  \n",
       "6     0.061576  \n",
       "7     0.058538  \n",
       "8     0.069265  \n",
       "9          NaN  \n",
       "10    0.084571  \n",
       "11    0.061089  \n",
       "12    0.062495  \n",
       "13    0.071861  \n",
       "14    0.035794  \n",
       "15         NaN  \n",
       "16    0.062495  \n",
       "17    0.080812  \n",
       "18    0.080369  \n",
       "19    0.093339  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kjv_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get the most similar scores without duplicating strongs words?\n",
    "\n",
    "Could use numpy.meshgrid to get all combinations of word indices for a verse, join to fa word and strongs word, calculate similarity for each combination.\n",
    "Need to then discard all but the set of complete indices that totals the highest similarity score. Not sure how to do that yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [0, 2],\n",
       "       [0, 3],\n",
       "       [0, 4],\n",
       "       [0, 5],\n",
       "       [0, 6],\n",
       "       [0, 7],\n",
       "       [0, 8],\n",
       "       [0, 9],\n",
       "       [1, 0],\n",
       "       [1, 1],\n",
       "       [1, 2],\n",
       "       [1, 3],\n",
       "       [1, 4],\n",
       "       [1, 5],\n",
       "       [1, 6],\n",
       "       [1, 7],\n",
       "       [1, 8],\n",
       "       [1, 9],\n",
       "       [2, 0],\n",
       "       [2, 1],\n",
       "       [2, 2],\n",
       "       [2, 3],\n",
       "       [2, 4],\n",
       "       [2, 5],\n",
       "       [2, 6],\n",
       "       [2, 7],\n",
       "       [2, 8],\n",
       "       [2, 9],\n",
       "       [3, 0],\n",
       "       [3, 1],\n",
       "       [3, 2],\n",
       "       [3, 3],\n",
       "       [3, 4],\n",
       "       [3, 5],\n",
       "       [3, 6],\n",
       "       [3, 7],\n",
       "       [3, 8],\n",
       "       [3, 9],\n",
       "       [4, 0],\n",
       "       [4, 1],\n",
       "       [4, 2],\n",
       "       [4, 3],\n",
       "       [4, 4],\n",
       "       [4, 5],\n",
       "       [4, 6],\n",
       "       [4, 7],\n",
       "       [4, 8],\n",
       "       [4, 9],\n",
       "       [5, 0],\n",
       "       [5, 1],\n",
       "       [5, 2],\n",
       "       [5, 3],\n",
       "       [5, 4],\n",
       "       [5, 5],\n",
       "       [5, 6],\n",
       "       [5, 7],\n",
       "       [5, 8],\n",
       "       [5, 9],\n",
       "       [6, 0],\n",
       "       [6, 1],\n",
       "       [6, 2],\n",
       "       [6, 3],\n",
       "       [6, 4],\n",
       "       [6, 5],\n",
       "       [6, 6],\n",
       "       [6, 7],\n",
       "       [6, 8],\n",
       "       [6, 9],\n",
       "       [7, 0],\n",
       "       [7, 1],\n",
       "       [7, 2],\n",
       "       [7, 3],\n",
       "       [7, 4],\n",
       "       [7, 5],\n",
       "       [7, 6],\n",
       "       [7, 7],\n",
       "       [7, 8],\n",
       "       [7, 9],\n",
       "       [8, 0],\n",
       "       [8, 1],\n",
       "       [8, 2],\n",
       "       [8, 3],\n",
       "       [8, 4],\n",
       "       [8, 5],\n",
       "       [8, 6],\n",
       "       [8, 7],\n",
       "       [8, 8],\n",
       "       [8, 9],\n",
       "       [9, 0],\n",
       "       [9, 1],\n",
       "       [9, 2],\n",
       "       [9, 3],\n",
       "       [9, 4],\n",
       "       [9, 5],\n",
       "       [9, 6],\n",
       "       [9, 7],\n",
       "       [9, 8],\n",
       "       [9, 9]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "gen1_word_idx_fa = nmv_df.loc[(nmv_df.book == \"Genesis\") & (nmv_df.idx_chapter == 0) & (nmv_df.idx_verse == 0),\"idx_word\"].values\n",
    "\n",
    "gen1_word_idx_en = esv_df.loc[(esv_df.book == \"Genesis\") & (esv_df.idx_chapter == 0) & (esv_df.idx_verse == 0),\"idx_word\"].values\n",
    "np.array(np.meshgrid(gen1_word_idx_fa, gen1_word_idx_en)).T.reshape(-1,2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Bible JSON from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "bible = requests.get(\"https://raw.githubusercontent.com/syncbible/syncbible/gh-pages/bibles/original.json\").json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['ב/ראשית', 'Hb/H7225', 'HR/Ncfsa'],\n",
       "  ['ברא', 'H1254', 'HVqp3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['ה/שמים', 'Hd/H8064', 'HTd/Ncmpa'],\n",
       "  ['ו/את', 'Hc/H853', 'HC/To'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa']],\n",
       " [['ו/ה/ארץ', 'Hc/Hd/H776', 'HC/Td/Ncbsa'],\n",
       "  ['היתה', 'H1961', 'HVqp3fs'],\n",
       "  ['תהו', 'H8414', 'HNcmsa'],\n",
       "  ['ו/בהו', 'Hc/H922', 'HC/Ncmsa'],\n",
       "  ['ו/חשך', 'Hc/H2822', 'HC/Ncmsa'],\n",
       "  ['על', 'H5921', 'HR'],\n",
       "  ['פני', 'H6440', 'HNcbpc'],\n",
       "  ['תהום', 'H8415', 'HNcbsa'],\n",
       "  ['ו/רוח', 'Hc/H7307', 'HC/Ncbsc'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['מרחפת', 'H7363', 'HVprfsa'],\n",
       "  ['על', 'H5921', 'HR'],\n",
       "  ['פני', 'H6440', 'HNcbpc'],\n",
       "  ['ה/מים', 'Hd/H4325', 'HTd/Ncmpa']],\n",
       " [['ו/יאמר', 'Hc/H559', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['יהי', 'H1961', 'HVqj3ms'],\n",
       "  ['אור', 'H216', 'HNcbsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['אור', 'H216', 'HNcbsa']],\n",
       " [['ו/ירא', 'Hc/H7200', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['ה/אור', 'Hd/H216', 'HTd/Ncbsa'],\n",
       "  ['כי', 'H3588', 'HC'],\n",
       "  ['טוב', 'H2896', 'HAamsa'],\n",
       "  ['ו/יבדל', 'Hc/H914', 'HC/Vhw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['בין', 'H996', 'HR'],\n",
       "  ['ה/אור', 'Hd/H216', 'HTd/Ncbsa'],\n",
       "  ['ו/בין', 'Hc/H996', 'HC/R'],\n",
       "  ['ה/חשך', 'Hd/H2822', 'HTd/Ncmsa']],\n",
       " [['ו/יקרא', 'Hc/H7121', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['ל/אור', 'Hl/H216', 'HRd/Ncbsa'],\n",
       "  ['יום', 'H3117', 'HNcmsa'],\n",
       "  ['ו/ל/חשך', 'Hc/Hl/H2822', 'HC/Rd/Ncmsa'],\n",
       "  ['קרא', 'H7121', 'HVqp3ms'],\n",
       "  ['לילה', 'H3915', 'HNcmsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['ערב', 'H6153', 'HNcmsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['בקר', 'H1242', 'HNcmsa'],\n",
       "  ['יום', 'H3117', 'HNcmsa'],\n",
       "  ['אחד', 'H259', 'HAcmsa']],\n",
       " [['ו/יאמר', 'Hc/H559', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['יהי', 'H1961', 'HVqj3ms'],\n",
       "  ['רקיע', 'H7549', 'HNcmsa'],\n",
       "  ['ב/תוך', 'Hb/H8432', 'HR/Ncmsc'],\n",
       "  ['ה/מים', 'Hd/H4325', 'HTd/Ncmpa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqi3ms'],\n",
       "  ['מבדיל', 'H914', 'HVhrmsa'],\n",
       "  ['בין', 'H996', 'HR'],\n",
       "  ['מים', 'H4325', 'HNcmpa'],\n",
       "  ['ל/מים', 'Hl/H4325', 'HR/Ncmpa']],\n",
       " [['ו/יעש', 'Hc/H6213', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['ה/רקיע', 'Hd/H7549', 'HTd/Ncmsa'],\n",
       "  ['ו/יבדל', 'Hc/H914', 'HC/Vhw3ms'],\n",
       "  ['בין', 'H996', 'HR'],\n",
       "  ['ה/מים', 'Hd/H4325', 'HTd/Ncmpa'],\n",
       "  ['אשר', 'H834', 'HTr'],\n",
       "  ['מ/תחת', 'Hm/H8478', 'HR/R'],\n",
       "  ['ל/רקיע', 'Hl/H7549', 'HRd/Ncmsa'],\n",
       "  ['ו/בין', 'Hc/H996', 'HC/R'],\n",
       "  ['ה/מים', 'Hd/H4325', 'HTd/Ncmpa'],\n",
       "  ['אשר', 'H834', 'HTr'],\n",
       "  ['מ/על', 'Hm/H5921', 'HR/R'],\n",
       "  ['ל/רקיע', 'Hl/H7549', 'HRd/Ncmsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['כן', 'H3651', 'HD']],\n",
       " [['ו/יקרא', 'Hc/H7121', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['ל/רקיע', 'Hl/H7549', 'HRd/Ncmsa'],\n",
       "  ['שמים', 'H8064', 'HNcmpa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['ערב', 'H6153', 'HNcmsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['בקר', 'H1242', 'HNcmsa'],\n",
       "  ['יום', 'H3117', 'HNcmsa'],\n",
       "  ['שני', 'H8145', 'HAomsa']],\n",
       " [['ו/יאמר', 'Hc/H559', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['יקוו', 'H6960', 'HVNj3mp'],\n",
       "  ['ה/מים', 'Hd/H4325', 'HTd/Ncmpa'],\n",
       "  ['מ/תחת', 'Hm/H8478', 'HR/R'],\n",
       "  ['ה/שמים', 'Hd/H8064', 'HTd/Ncmpa'],\n",
       "  ['אל', 'H413', 'HR'],\n",
       "  ['מקום', 'H4725', 'HNcmsa'],\n",
       "  ['אחד', 'H259', 'HAcmsa'],\n",
       "  ['ו/תראה', 'Hc/H7200', 'HC/VNi3fs'],\n",
       "  ['ה/יבשה', 'Hd/H3004', 'HTd/Ncfsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['כן', 'H3651', 'HD']],\n",
       " [['ו/יקרא', 'Hc/H7121', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['ל/יבשה', 'Hl/H3004', 'HRd/Ncfsa'],\n",
       "  ['ארץ', 'H776', 'HNcbsa'],\n",
       "  ['ו/ל/מקוה', 'Hc/Hl/H4723', 'HC/R/Ncmsc'],\n",
       "  ['ה/מים', 'Hd/H4325', 'HTd/Ncmpa'],\n",
       "  ['קרא', 'H7121', 'HVqp3ms'],\n",
       "  ['ימים', 'H3220', 'HNcmpa'],\n",
       "  ['ו/ירא', 'Hc/H7200', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['כי', 'H3588', 'HC'],\n",
       "  ['טוב', 'H2896', 'HAamsa']],\n",
       " [['ו/יאמר', 'Hc/H559', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['תדשא', 'H1876', 'HVhj3fs'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['דשא', 'H1877', 'HNcmsa'],\n",
       "  ['עשב', 'H6212', 'HNcmsa'],\n",
       "  ['מזריע', 'H2232', 'HVhrmsa'],\n",
       "  ['זרע', 'H2233', 'HNcmsa'],\n",
       "  ['עץ', 'H6086', 'HNcmsc'],\n",
       "  ['פרי', 'H6529', 'HNcmsa'],\n",
       "  ['עשה', 'H6213', 'HVqrmsa'],\n",
       "  ['פרי', 'H6529', 'HNcmsa'],\n",
       "  ['ל/מינ/ו', 'Hl/H4327', 'HR/Ncmsc/Sp3ms'],\n",
       "  ['אשר', 'H834', 'HTr'],\n",
       "  ['זרע/ו', 'H2233', 'HNcmsc/Sp3ms'],\n",
       "  ['ב/ו', 'Hb', 'HR/Sp3ms'],\n",
       "  ['על', 'H5921', 'HR'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['כן', 'H3651', 'HD']],\n",
       " [['ו/תוצא', 'Hc/H3318', 'HC/Vhw3fs'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['דשא', 'H1877', 'HNcmsa'],\n",
       "  ['עשב', 'H6212', 'HNcmsa'],\n",
       "  ['מזריע', 'H2232', 'HVhrmsa'],\n",
       "  ['זרע', 'H2233', 'HNcmsa'],\n",
       "  ['ל/מינ/הו', 'Hl/H4327', 'HR/Ncmsc/Sp3ms'],\n",
       "  ['ו/עץ', 'Hc/H6086', 'HC/Ncmsa'],\n",
       "  ['עשה', 'H6213', 'HVqrmsa'],\n",
       "  ['פרי', 'H6529', 'HNcmsa'],\n",
       "  ['אשר', 'H834', 'HTr'],\n",
       "  ['זרע/ו', 'H2233', 'HNcmsc/Sp3ms'],\n",
       "  ['ב/ו', 'Hb', 'HR/Sp3ms'],\n",
       "  ['ל/מינ/הו', 'Hl/H4327', 'HR/Ncmsc/Sp3ms'],\n",
       "  ['ו/ירא', 'Hc/H7200', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['כי', 'H3588', 'HC'],\n",
       "  ['טוב', 'H2896', 'HAamsa']],\n",
       " [['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['ערב', 'H6153', 'HNcmsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['בקר', 'H1242', 'HNcmsa'],\n",
       "  ['יום', 'H3117', 'HNcmsa'],\n",
       "  ['שלישי', 'H7992', 'HAomsa']],\n",
       " [['ו/יאמר', 'Hc/H559', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['יהי', 'H1961', 'HVqj3ms'],\n",
       "  ['מארת', 'H3974', 'HNcmpa'],\n",
       "  ['ב/רקיע', 'Hb/H7549', 'HR/Ncmsc'],\n",
       "  ['ה/שמים', 'Hd/H8064', 'HTd/Ncmpa'],\n",
       "  ['ל/הבדיל', 'Hl/H914', 'HR/Vhc'],\n",
       "  ['בין', 'H996', 'HR'],\n",
       "  ['ה/יום', 'Hd/H3117', 'HTd/Ncmsa'],\n",
       "  ['ו/בין', 'Hc/H996', 'HC/R'],\n",
       "  ['ה/לילה', 'Hd/H3915', 'HTd/Ncmsa'],\n",
       "  ['ו/היו', 'Hc/H1961', 'HC/Vqq3cp'],\n",
       "  ['ל/אתת', 'Hl/H226', 'HR/Ncbpa'],\n",
       "  ['ו/ל/מועדים', 'Hc/Hl/H4150', 'HC/R/Ncmpa'],\n",
       "  ['ו/ל/ימים', 'Hc/Hl/H3117', 'HC/R/Ncmpa'],\n",
       "  ['ו/שנים', 'Hc/H8141', 'HC/Ncfpa']],\n",
       " [['ו/היו', 'Hc/H1961', 'HC/Vqq3cp'],\n",
       "  ['ל/מאורת', 'Hl/H3974', 'HR/Ncmpa'],\n",
       "  ['ב/רקיע', 'Hb/H7549', 'HR/Ncmsc'],\n",
       "  ['ה/שמים', 'Hd/H8064', 'HTd/Ncmpa'],\n",
       "  ['ל/האיר', 'Hl/H215', 'HR/Vhc'],\n",
       "  ['על', 'H5921', 'HR'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['כן', 'H3651', 'HD']],\n",
       " [['ו/יעש', 'Hc/H6213', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['שני', 'H8147', 'HAcmdc'],\n",
       "  ['ה/מארת', 'Hd/H3974', 'HTd/Ncmpa'],\n",
       "  ['ה/גדלים', 'Hd/H1419', 'HTd/Aampa'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['ה/מאור', 'Hd/H3974', 'HTd/Ncmsa'],\n",
       "  ['ה/גדל', 'Hd/H1419', 'HTd/Aamsa'],\n",
       "  ['ל/ממשלת', 'Hl/H4475', 'HR/Ncbsc'],\n",
       "  ['ה/יום', 'Hd/H3117', 'HTd/Ncmsa'],\n",
       "  ['ו/את', 'Hc/H853', 'HC/To'],\n",
       "  ['ה/מאור', 'Hd/H3974', 'HTd/Ncmsa'],\n",
       "  ['ה/קטן', 'Hd/H6996', 'HTd/Aamsa'],\n",
       "  ['ל/ממשלת', 'Hl/H4475', 'HR/Ncbsc'],\n",
       "  ['ה/לילה', 'Hd/H3915', 'HTd/Ncmsa'],\n",
       "  ['ו/את', 'Hc/H853', 'HC/To'],\n",
       "  ['ה/כוכבים', 'Hd/H3556', 'HTd/Ncmpa']],\n",
       " [['ו/יתן', 'Hc/H5414', 'HC/Vqw3ms'],\n",
       "  ['את/ם', 'H853', 'HTo/Sp3mp'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['ב/רקיע', 'Hb/H7549', 'HR/Ncmsc'],\n",
       "  ['ה/שמים', 'Hd/H8064', 'HTd/Ncmpa'],\n",
       "  ['ל/האיר', 'Hl/H215', 'HR/Vhc'],\n",
       "  ['על', 'H5921', 'HR'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa']],\n",
       " [['ו/ל/משל', 'Hc/Hl/H4910', 'HC/R/Vqc'],\n",
       "  ['ב/יום', 'Hb/H3117', 'HRd/Ncmsa'],\n",
       "  ['ו/ב/לילה', 'Hc/Hb/H3915', 'HC/Rd/Ncmsa'],\n",
       "  ['ו/ל/הבדיל', 'Hc/Hl/H914', 'HC/R/Vhc'],\n",
       "  ['בין', 'H996', 'HR'],\n",
       "  ['ה/אור', 'Hd/H216', 'HTd/Ncbsa'],\n",
       "  ['ו/בין', 'Hc/H996', 'HC/R'],\n",
       "  ['ה/חשך', 'Hd/H2822', 'HTd/Ncmsa'],\n",
       "  ['ו/ירא', 'Hc/H7200', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['כי', 'H3588', 'HC'],\n",
       "  ['טוב', 'H2896', 'HAamsa']],\n",
       " [['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['ערב', 'H6153', 'HNcmsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['בקר', 'H1242', 'HNcmsa'],\n",
       "  ['יום', 'H3117', 'HNcmsa'],\n",
       "  ['רביעי', 'H7243', 'HAomsa']],\n",
       " [['ו/יאמר', 'Hc/H559', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['ישרצו', 'H8317', 'HVqi3mp'],\n",
       "  ['ה/מים', 'Hd/H4325', 'HTd/Ncmpa'],\n",
       "  ['שרץ', 'H8318', 'HNcmsc'],\n",
       "  ['נפש', 'H5315', 'HNcbsa'],\n",
       "  ['חיה', 'H2416', 'HAafsa'],\n",
       "  ['ו/עוף', 'Hc/H5775', 'HC/Ncmsa'],\n",
       "  ['יעופף', 'H5774', 'HVoi3ms'],\n",
       "  ['על', 'H5921', 'HR'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['על', 'H5921', 'HR'],\n",
       "  ['פני', 'H6440', 'HNcbpc'],\n",
       "  ['רקיע', 'H7549', 'HNcmsc'],\n",
       "  ['ה/שמים', 'Hd/H8064', 'HTd/Ncmpa']],\n",
       " [['ו/יברא', 'Hc/H1254', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['ה/תנינם', 'Hd/H8577', 'HTd/Ncmpa'],\n",
       "  ['ה/גדלים', 'Hd/H1419', 'HTd/Aampa'],\n",
       "  ['ו/את', 'Hc/H853', 'HC/To'],\n",
       "  ['כל', 'H3605', 'HNcmsc'],\n",
       "  ['נפש', 'H5315', 'HNcbsa'],\n",
       "  ['ה/חיה', 'Hd/H2416', 'HTd/Aafsa'],\n",
       "  ['ה/רמשת', 'Hd/H7430', 'HTd/Vqrfsa'],\n",
       "  ['אשר', 'H834', 'HTr'],\n",
       "  ['שרצו', 'H8317', 'HVqp3cp'],\n",
       "  ['ה/מים', 'Hd/H4325', 'HTd/Ncmpa'],\n",
       "  ['ל/מינ/הם', 'Hl/H4327', 'HR/Ncmsc/Sp3mp'],\n",
       "  ['ו/את', 'Hc/H853', 'HC/To'],\n",
       "  ['כל', 'H3605', 'HNcmsc'],\n",
       "  ['עוף', 'H5775', 'HNcmsa'],\n",
       "  ['כנף', 'H3671', 'HNcfsa'],\n",
       "  ['ל/מינ/הו', 'Hl/H4327', 'HR/Ncmsc/Sp3ms'],\n",
       "  ['ו/ירא', 'Hc/H7200', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['כי', 'H3588', 'HC'],\n",
       "  ['טוב', 'H2896', 'HAamsa']],\n",
       " [['ו/יברך', 'Hc/H1288', 'HC/Vpw3ms'],\n",
       "  ['את/ם', 'H853', 'HTo/Sp3mp'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['ל/אמר', 'Hl/H559', 'HR/Vqc'],\n",
       "  ['פרו', 'H6509', 'HVqv2mp'],\n",
       "  ['ו/רבו', 'Hc/H7235', 'HC/Vqv2mp'],\n",
       "  ['ו/מלאו', 'Hc/H4390', 'HC/Vqv2mp'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['ה/מים', 'Hd/H4325', 'HTd/Ncmpa'],\n",
       "  ['ב/ימים', 'Hb/H3220', 'HRd/Ncmpa'],\n",
       "  ['ו/ה/עוף', 'Hc/Hd/H5775', 'HC/Td/Ncmsa'],\n",
       "  ['ירב', 'H7235', 'HVqj3ms'],\n",
       "  ['ב/ארץ', 'Hb/H776', 'HRd/Ncbsa']],\n",
       " [['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['ערב', 'H6153', 'HNcmsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['בקר', 'H1242', 'HNcmsa'],\n",
       "  ['יום', 'H3117', 'HNcmsa'],\n",
       "  ['חמישי', 'H2549', 'HAomsa']],\n",
       " [['ו/יאמר', 'Hc/H559', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['תוצא', 'H3318', 'HVhj3fs'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['נפש', 'H5315', 'HNcbsa'],\n",
       "  ['חיה', 'H2416', 'HAafsa'],\n",
       "  ['ל/מינ/ה', 'Hl/H4327', 'HR/Ncmsc/Sp3fs'],\n",
       "  ['בהמה', 'H929', 'HNcfsa'],\n",
       "  ['ו/רמש', 'Hc/H7431', 'HC/Ncmsa'],\n",
       "  ['ו/חיתו', 'Hc/H2416', 'HC/Ncfsc'],\n",
       "  ['ארץ', 'H776', 'HNcbsa'],\n",
       "  ['ל/מינ/ה', 'Hl/H4327', 'HR/Ncmsc/Sp3fs'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['כן', 'H3651', 'HD']],\n",
       " [['ו/יעש', 'Hc/H6213', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['חית', 'H2416', 'HNcfsc'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['ל/מינ/ה', 'Hl/H4327', 'HR/Ncmsc/Sp3fs'],\n",
       "  ['ו/את', 'Hc/H853', 'HC/To'],\n",
       "  ['ה/בהמה', 'Hd/H929', 'HTd/Ncfsa'],\n",
       "  ['ל/מינ/ה', 'Hl/H4327', 'HR/Ncmsc/Sp3fs'],\n",
       "  ['ו/את', 'Hc/H853', 'HC/To'],\n",
       "  ['כל', 'H3605', 'HNcmsc'],\n",
       "  ['רמש', 'H7431', 'HNcmsc'],\n",
       "  ['ה/אדמה', 'Hd/H127', 'HTd/Ncfsa'],\n",
       "  ['ל/מינ/הו', 'Hl/H4327', 'HR/Ncmsc/Sp3ms'],\n",
       "  ['ו/ירא', 'Hc/H7200', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['כי', 'H3588', 'HC'],\n",
       "  ['טוב', 'H2896', 'HAamsa']],\n",
       " [['ו/יאמר', 'Hc/H559', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['נעשה', 'H6213', 'HVqi1cp'],\n",
       "  ['אדם', 'H120', 'HNcmsa'],\n",
       "  ['ב/צלמ/נו', 'Hb/H6754', 'HR/Ncmsc/Sp1cp'],\n",
       "  ['כ/דמות/נו', 'Hk/H1823', 'HR/Ncfsc/Sp1cp'],\n",
       "  ['ו/ירדו', 'Hc/H7287', 'HC/Vqj3mp'],\n",
       "  ['ב/דגת', 'Hb/H1710', 'HR/Ncfsc'],\n",
       "  ['ה/ים', 'Hd/H3220', 'HTd/Ncmsa'],\n",
       "  ['ו/ב/עוף', 'Hc/Hb/H5775', 'HC/R/Ncmsc'],\n",
       "  ['ה/שמים', 'Hd/H8064', 'HTd/Ncmpa'],\n",
       "  ['ו/ב/בהמה', 'Hc/Hb/H929', 'HC/Rd/Ncfsa'],\n",
       "  ['ו/ב/כל', 'Hc/Hb/H3605', 'HC/R/Ncmsc'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['ו/ב/כל', 'Hc/Hb/H3605', 'HC/R/Ncmsc'],\n",
       "  ['ה/רמש', 'Hd/H7431', 'HTd/Ncmsa'],\n",
       "  ['ה/רמש', 'Hd/H7430', 'HTd/Vqrmsa'],\n",
       "  ['על', 'H5921', 'HR'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa']],\n",
       " [['ו/יברא', 'Hc/H1254', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['ה/אדם', 'Hd/H120', 'HTd/Ncmsa'],\n",
       "  ['ב/צלמ/ו', 'Hb/H6754', 'HR/Ncmsc/Sp3ms'],\n",
       "  ['ב/צלם', 'Hb/H6754', 'HR/Ncmsc'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['ברא', 'H1254', 'HVqp3ms'],\n",
       "  ['את/ו', 'H853', 'HTo/Sp3ms'],\n",
       "  ['זכר', 'H2145', 'HAamsa'],\n",
       "  ['ו/נקבה', 'Hc/H5347', 'HC/Ncfsa'],\n",
       "  ['ברא', 'H1254', 'HVqp3ms'],\n",
       "  ['את/ם', 'H853', 'HTo/Sp3mp']],\n",
       " [['ו/יברך', 'Hc/H1288', 'HC/Vpw3ms'],\n",
       "  ['את/ם', 'H853', 'HTo/Sp3mp'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['ו/יאמר', 'Hc/H559', 'HC/Vqw3ms'],\n",
       "  ['ל/הם', 'Hl', 'HR/Sp3mp'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['פרו', 'H6509', 'HVqv2mp'],\n",
       "  ['ו/רבו', 'Hc/H7235', 'HC/Vqv2mp'],\n",
       "  ['ו/מלאו', 'Hc/H4390', 'HC/Vqv2mp'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['ו/כבש/ה', 'Hc/H3533', 'HC/Vqv2mp/Sp3fs'],\n",
       "  ['ו/רדו', 'Hc/H7287', 'HC/Vqv2mp'],\n",
       "  ['ב/דגת', 'Hb/H1710', 'HR/Ncfsc'],\n",
       "  ['ה/ים', 'Hd/H3220', 'HTd/Ncmsa'],\n",
       "  ['ו/ב/עוף', 'Hc/Hb/H5775', 'HC/R/Ncmsc'],\n",
       "  ['ה/שמים', 'Hd/H8064', 'HTd/Ncmpa'],\n",
       "  ['ו/ב/כל', 'Hc/Hb/H3605', 'HC/R/Ncmsc'],\n",
       "  ['חיה', 'H2416', 'HNcbsa'],\n",
       "  ['ה/רמשת', 'Hd/H7430', 'HTd/Vqrfsa'],\n",
       "  ['על', 'H5921', 'HR'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa']],\n",
       " [['ו/יאמר', 'Hc/H559', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['הנה', 'H2009', 'HTm'],\n",
       "  ['נתתי', 'H5414', 'HVqp1cs'],\n",
       "  ['ל/כם', 'Hl', 'HR/Sp2mp'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['כל', 'H3605', 'HNcmsc'],\n",
       "  ['עשב', 'H6212', 'HNcmsa'],\n",
       "  ['זרע', 'H2232', 'HVqrmsa'],\n",
       "  ['זרע', 'H2233', 'HNcmsa'],\n",
       "  ['אשר', 'H834', 'HTr'],\n",
       "  ['על', 'H5921', 'HR'],\n",
       "  ['פני', 'H6440', 'HNcbpc'],\n",
       "  ['כל', 'H3605', 'HNcmsc'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['ו/את', 'Hc/H853', 'HC/To'],\n",
       "  ['כל', 'H3605', 'HNcmsc'],\n",
       "  ['ה/עץ', 'Hd/H6086', 'HTd/Ncmsa'],\n",
       "  ['אשר', 'H834', 'HTr'],\n",
       "  ['ב/ו', 'Hb', 'HR/Sp3ms'],\n",
       "  ['פרי', 'H6529', 'HNcmsc'],\n",
       "  ['עץ', 'H6086', 'HNcmsa'],\n",
       "  ['זרע', 'H2232', 'HVqrmsa'],\n",
       "  ['זרע', 'H2233', 'HNcmsa'],\n",
       "  ['ל/כם', 'Hl', 'HR/Sp2mp'],\n",
       "  ['יהיה', 'H1961', 'HVqi3ms'],\n",
       "  ['ל/אכלה', 'Hl/H402', 'HR/Ncfsa']],\n",
       " [['ו/ל/כל', 'Hc/Hl/H3605', 'HC/R/Ncmsc'],\n",
       "  ['חית', 'H2416', 'HNcfsc'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['ו/ל/כל', 'Hc/Hl/H3605', 'HC/R/Ncmsc'],\n",
       "  ['עוף', 'H5775', 'HNcmsc'],\n",
       "  ['ה/שמים', 'Hd/H8064', 'HTd/Ncmpa'],\n",
       "  ['ו/ל/כל', 'Hc/Hl/H3605', 'HC/R/Ncmsc'],\n",
       "  ['רומש', 'H7430', 'HVqrmsa'],\n",
       "  ['על', 'H5921', 'HR'],\n",
       "  ['ה/ארץ', 'Hd/H776', 'HTd/Ncbsa'],\n",
       "  ['אשר', 'H834', 'HTr'],\n",
       "  ['ב/ו', 'Hb', 'HR/Sp3ms'],\n",
       "  ['נפש', 'H5315', 'HNcbsa'],\n",
       "  ['חיה', 'H2416', 'HAafsa'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['כל', 'H3605', 'HNcmsc'],\n",
       "  ['ירק', 'H3418', 'HNcmsa'],\n",
       "  ['עשב', 'H6212', 'HNcmsa'],\n",
       "  ['ל/אכלה', 'Hl/H402', 'HR/Ncfsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['כן', 'H3651', 'HD']],\n",
       " [['ו/ירא', 'Hc/H7200', 'HC/Vqw3ms'],\n",
       "  ['אלהים', 'H430', 'HNcmpa'],\n",
       "  ['את', 'H853', 'HTo'],\n",
       "  ['כל', 'H3605', 'HNcmsc'],\n",
       "  ['אשר', 'H834', 'HTr'],\n",
       "  ['עשה', 'H6213', 'HVqp3ms'],\n",
       "  ['ו/הנה', 'Hc/H2009', 'HC/Tm'],\n",
       "  ['טוב', 'H2896', 'HAamsa'],\n",
       "  ['מאד', 'H3966', 'HD'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['ערב', 'H6153', 'HNcmsa'],\n",
       "  ['ו/יהי', 'Hc/H1961', 'HC/Vqw3ms'],\n",
       "  ['בקר', 'H1242', 'HNcmsa'],\n",
       "  ['יום', 'H3117', 'HNcmsc'],\n",
       "  ['ה/ששי', 'Hd/H8345', 'HTd/Aomsa']]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible[\"books\"][\"Genesis\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"inputs/original.json\", \"w\") as f:\n",
    "    json.dump(bible, f)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "107f9aec7514d8e79cd4921b65cc44c9175ae350aa3c0c7c5debc04712a53a45"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('venv310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
